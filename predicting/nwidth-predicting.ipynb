{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Network per channel\n\nAs we dive more precisely into the topic, we now create a more robust dataset:\n\n* each input is an image of size `(C, W, H)`. On each channel, there is a grayscale sliced image. So, \n  * the first `C/3` channels are sliced images along x axis\n  * the following `C/3` channels are sliced images along x axis\n  * the last `C/3` channels are sliced images along x axis\n* each output is list of fabric descriptors\n\nIf we take `C=3`, we can use a pretrained VGG model. Indeed, this model is trained on RGB images, which have 3 channels.","metadata":{}},{"cell_type":"markdown","source":"# Importing the dataframe","metadata":{}},{"cell_type":"markdown","source":"Firstly, we initialize wandb. It is a tool that allows to store the losses and retrieve the deframe. Otherwise, you can directly access locally the dataframe on your computer.","metadata":{}},{"cell_type":"code","source":"!pip install wandb --upgrade","metadata":{"executionInfo":{"elapsed":4758,"status":"ok","timestamp":1644495641039,"user":{"displayName":"Matias Etcheverry","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAaRgyKc_t8sZam6khtkO8IvEB4xqvz8dwKcJh=s64","userId":"13887759295001408411"},"user_tz":-60},"id":"KCF0gEhbI9yg","outputId":"65a442db-4731-422d-a7a4-ae8f237f375d","execution":{"iopub.status.busy":"2022-04-14T06:49:17.392157Z","iopub.execute_input":"2022-04-14T06:49:17.393043Z","iopub.status.idle":"2022-04-14T06:49:31.994329Z","shell.execute_reply.started":"2022-04-14T06:49:17.392941Z","shell.execute_reply":"2022-04-14T06:49:31.993498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We import all the useful packages.","metadata":{}},{"cell_type":"code","source":"import sys\nfrom pathlib import Path\n\nIS_COLAB = \"google.colab\" in sys.modules\nIS_KAGGLE = \"kaggle_secrets\" in sys.modules\nif IS_KAGGLE:\n    repo_path = Path(\"../input/microstructure-reconstruction\")\nelif IS_COLAB:\n    from google.colab import drive\n\n    drive.mount(\"/content/gdrive\")\n    repo_path = Path(\"/content/gdrive/MyDrive/microstructure-reconstruction\")\nelse:\n    repo_path = Path(\"/home/matias/microstructure-reconstruction\")\nsys.path.append(str(repo_path))\n\nfrom copy import deepcopy\nfrom importlib import reload\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\nimport pandas as pd\nimport pytorch_lightning as pl\nimport torch\nfrom typing import Union, List\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchmetrics\nimport torchvision.models as pretrained_models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KernelDensity\nfrom sklearn.preprocessing import MinMaxScaler\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms, utils\nfrom tqdm import tqdm\n\nimport wandb\nfrom custom_datasets import dataset\nfrom custom_models import models\nfrom tools import dataframe_reformat, inspect_code, plotting, training, wandb_api\n\nlog_wandb = True\n\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\nkwargs = {\"num_workers\": 2, \"pin_memory\": True} if use_cuda else {\"num_workers\": 4}\nprint(f\"[INFO]: Computation device: {device}\")\n","metadata":{"executionInfo":{"elapsed":12323,"status":"ok","timestamp":1644495653358,"user":{"displayName":"Matias Etcheverry","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAaRgyKc_t8sZam6khtkO8IvEB4xqvz8dwKcJh=s64","userId":"13887759295001408411"},"user_tz":-60},"id":"396f8318","outputId":"ecbf203b-7c9f-4285-a787-3c279f8be90f","execution":{"iopub.status.busy":"2022-04-14T06:49:31.999565Z","iopub.execute_input":"2022-04-14T06:49:31.999816Z","iopub.status.idle":"2022-04-14T06:49:36.628746Z","shell.execute_reply.started":"2022-04-14T06:49:31.999786Z","shell.execute_reply":"2022-04-14T06:49:36.627974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We initialize a wandb run, that will save our metrics","metadata":{}},{"cell_type":"code","source":"if log_wandb:\n    import wandb\n\n    wandb_api.login()\n    run = wandb.init(\n        project=\"microstructure-reconstruction\",\n        group=\"NWidth Network\",\n        job_type=\"test\",\n    )\n","metadata":{"executionInfo":{"elapsed":6825,"status":"ok","timestamp":1644495660175,"user":{"displayName":"Matias Etcheverry","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAaRgyKc_t8sZam6khtkO8IvEB4xqvz8dwKcJh=s64","userId":"13887759295001408411"},"user_tz":-60},"id":"dj85A2mMIz_j","outputId":"91b1c0e2-dbca-4381-e5ac-9f404baf57db","execution":{"iopub.status.busy":"2022-04-14T06:49:36.630211Z","iopub.execute_input":"2022-04-14T06:49:36.630426Z","iopub.status.idle":"2022-04-14T06:49:47.043446Z","shell.execute_reply.started":"2022-04-14T06:49:36.630399Z","shell.execute_reply":"2022-04-14T06:49:47.04275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Parameters of our run:","metadata":{}},{"cell_type":"code","source":"if log_wandb:\n    config = wandb.config\nelse:\n    config = {}\n\nconfig[\"job_type\"] = run.job_type\nconfig[\"train_val_split\"] = 0.7\nconfig[\"seed\"] = 42\nconfig[\"batch_size\"] = 64\nconfig[\"learning_rate\"] = 0.0001\nconfig[\"device\"] = device\nconfig[\"momentum\"] = 0.9\nconfig[\"architecture\"] = \"pretrained VGG\"\nconfig[\"input_width\"] = 64\nconfig[\"weight_decay\"] = 0.00005\nconfig[\"epochs\"] = 0\nconfig[\"frac_sample\"] = 1\nconfig[\"frac_noise\"] = 0\nconfig[\"nb_image_per_axis\"] = 3\n# config[\"total_layers\"] = 24\n# config[\"fixed_layers\"] = 0\nconfig[\"log_wandb\"] = log_wandb\ntorch.manual_seed(config[\"seed\"])\npl.seed_everything(config[\"seed\"])","metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1644495660176,"user":{"displayName":"Matias Etcheverry","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAaRgyKc_t8sZam6khtkO8IvEB4xqvz8dwKcJh=s64","userId":"13887759295001408411"},"user_tz":-60},"id":"w2Me9kwpIz_j","outputId":"f4290858-5c72-4331-ed8d-b99ffeb3b058","execution":{"iopub.status.busy":"2022-04-14T06:49:47.048375Z","iopub.execute_input":"2022-04-14T06:49:47.050241Z","iopub.status.idle":"2022-04-14T06:49:47.075816Z","shell.execute_reply.started":"2022-04-14T06:49:47.050203Z","shell.execute_reply":"2022-04-14T06:49:47.075059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We retrieve the dataframe containing the descriptors. This can locally be done on your computer.","metadata":{}},{"cell_type":"code","source":"class DataModule(pl.LightningDataModule):\n    def __init__(\n        self,\n        config,\n        repo_path,\n        train_df=None,\n        test_df=None,\n        train_dataset=None,\n        validation_dataset=None,\n    ):\n        super().__init__()\n        self.config = config\n        self.repo_path = repo_path\n        self.train_df = train_df.convert_dtypes() if train_df is not None else None\n        self.test_df = test_df.convert_dtypes() if test_df is not None else None\n        self.train_dataset = train_dataset\n        self.validation_dataset = validation_dataset\n\n        if self.config[\"log_wandb\"]:\n            if self.train_df is None:\n                self.training_data_at = wandb.Api().artifact(\n                    f\"matiasetcheverry/microstructure-reconstruction/train_df:{self.config['nb_image_per_axis']}_images_invariants\"\n                )\n            if self.test_df is None:\n                self.test_data_at = wandb.Api().artifact(\n                    f\"matiasetcheverry/microstructure-reconstruction/test_df:{self.config['nb_image_per_axis']}_images_invariants\"\n                )\n\n        self.transform = transforms.Compose(\n            [\n                transforms.GaussianBlur(kernel_size=3, sigma=0.5),\n            ]\n        )\n\n    def prepare_data(self):\n        if self.config[\"log_wandb\"]:\n            if self.train_df is None:\n                self.training_data_at.download()\n            if self.test_df is None:\n                self.test_data_at.download()\n\n    def _init_df_wandb(self):\n        if self.train_df is None:\n            self.train_df = wandb_api.convert_table_to_dataframe(\n                self.training_data_at.get(\"fabrics\")\n            )\n            self.train_df[\"photos\"] = self.train_df[\"photos\"].apply(\n                func=lambda photo_paths: [\n                    str(self.repo_path / Path(x)) for x in photo_paths\n                ]\n            )\n        if self.test_df is None:\n            self.test_df = wandb_api.convert_table_to_dataframe(\n                self.test_data_at.get(\"fabrics\")\n            )\n            self.test_df[\"photos\"] = self.test_df[\"photos\"].apply(\n                func=lambda photo_paths: [\n                    str(self.repo_path / Path(x)) for x in photo_paths\n                ]\n            )\n\n    def _init_df_local(self):\n        fabrics_df = pd.read_csv(self.repo_path / \"REV1_600/fabrics.txt\")\n        path_to_slices = self.repo_path / \"REV1_600/REV1_600Slices\"\n        fabrics_df[\"photos\"] = fabrics_df[\"id\"].apply(\n            func=dataframe_reformat.associate_rev_id_to_its_images,\n            args=(path_to_slices, self.config[\"nb_image_per_axis\"]),\n        )\n        fabrics_df = fabrics_df[fabrics_df.photos.str.len().gt(0)]\n        fabrics_df[\"photos\"] = fabrics_df[\"photos\"].apply(func=lambda x: sorted(x))\n        train_df, test_df = train_test_split(\n            fabrics_df,\n            train_size=config[\"train_val_split\"],\n            random_state=config[\"seed\"],\n            shuffle=True,\n        )\n        if self.train_df is None:\n            self.train_df = train_df.reset_index(drop=True)\n        if self.test_df is None:\n            self.test_df = test_df.reset_index(drop=True)\n\n    def init_df(self):\n        if self.config[\"log_wandb\"]:\n            self._init_df_wandb()\n        else:\n            self._init_df_local()\n    \n    def setup(self, stage):\n        if self.train_dataset is None or self.validation_dataset is None:\n            self.init_df()\n            \n            self.scaler = MinMaxScaler(feature_range=(0, 1))\n            self.scaler.partial_fit(self.train_df.iloc[:, 1:-1])\n            self.scaler.partial_fit(self.test_df.iloc[:, 1:-1])\n            \n            normalized_train_df = deepcopy(self.train_df)\n            normalized_train_df.iloc[:, 1:-1] = self.scaler.transform(\n                self.train_df.iloc[:, 1:-1]\n            )\n            normalized_test_df = deepcopy(self.test_df)\n            normalized_test_df.iloc[:, 1:-1] = self.scaler.transform(\n                self.test_df.iloc[:, 1:-1]\n            )\n\n            if self.train_dataset is None:\n                self.train_dataset = dataset.NWidthStackedPhotosDataset(\n                    normalized_train_df,\n                    nb_image_per_axis=config[\"nb_image_per_axis\"],\n                    transform=self.transform,\n                    noise=0,\n                )\n            if self.validation_dataset is None:\n                self.validation_dataset = dataset.NWidthStackedPhotosDataset(\n                    normalized_test_df,\n                    nb_image_per_axis=config[\"nb_image_per_axis\"],\n                    transform=self.transform,\n                    noise=0,\n                )\n            self.targets = self.test_df.iloc[:, 1:-1].to_numpy()\n\n    def train_dataloader(self):\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.config[\"batch_size\"],\n            shuffle=True,\n            **kwargs,\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.validation_dataset,\n            batch_size=self.config[\"batch_size\"],\n            shuffle=False,\n            **kwargs,\n        )\n\n    def test_dataloader(self):\n        return self.val_dataloader()\n\n    def predict_dataloader(self):\n        return DataLoader(\n            [image for image, _ in self.validation_dataset],\n            batch_size=self.config[\"batch_size\"],\n            shuffle=False,\n            **kwargs,\n        )\n\ndm = DataModule(config, repo_path)\n\ndm.prepare_data()\ndm.setup(stage=\"fit\")\nfirst_batch = next(iter(dm.train_dataloader()))\nprint(\"Nb of descriptors:\", len(first_batch[1][0]))\nprint(\"Nb batch in dataset:\", len(dm.train_dataloader()))\nprint(\"Size of a batch:\", len(first_batch[0]))\nprint(\"Approx size of the dataset:\", len(first_batch[0])*len(dm.train_dataloader()))\nimages, labels = first_batch[0], first_batch[1]\nprint(\"Image shape:\", images[0].shape)\ngrid = utils.make_grid(images)\nfig = plt.figure(figsize=(40, 10))\nplt.imshow(grid.numpy().transpose((1, 2, 0)))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T06:49:47.079281Z","iopub.execute_input":"2022-04-14T06:49:47.079568Z","iopub.status.idle":"2022-04-14T06:50:08.349745Z","shell.execute_reply.started":"2022-04-14T06:49:47.079541Z","shell.execute_reply":"2022-04-14T06:50:08.348886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Network definition","metadata":{}},{"cell_type":"markdown","source":"The next step is to define our model. This model is inspired by VGG11:\n\n* we define several convulational blocks.\n* each of this block is sequence of:\n  * convulational layer with `kernel_size=3, padding=1`\n  * activation function, here it is the `ReLU`\n  * max pooling layer with `kernel_size=2, stride=2` which aims at reducing the size of the convolutional layers","metadata":{}},{"cell_type":"code","source":"class PreTrainedVGG(models.BaseModel):\n    def __init__(self, config, scaler=None):\n        super().__init__(config)\n\n        self.config = config\n        self.config[\"model_type\"] = type(self)\n        self.scaler = scaler\n\n        self.configure_model()\n        self.configure_criterion()\n        self.configure_metrics()\n\n    def configure_model(self):\n        assert self.config[\"total_layers\"] >= self.config[\"fixed_layers\"]\n        vgg = pretrained_models.vgg16_bn(pretrained=True)\n        self.layers = nn.Sequential(\n            *(list(vgg.features.children())[: self.config[\"total_layers\"]])\n        )\n        for idx, child in enumerate(self.layers.children()):\n            if idx < self.config[\"fixed_layers\"] and isinstance(child, nn.Conv2d):\n                for param in child.parameters():\n                    param.requires_grad = False\n#             else:\n#                 reset_parameters = getattr(child, \"reset_parameters\", None)\n#                 if callable(reset_parameters):\n#                     child.reset_parameters()\n#         self.max_pool = nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n        nb_channels, height, width = (\n            self.layers(\n                torch.rand(\n                    (1, 3, self.config[\"input_width\"], config[\"nb_image_per_axis\"]*self.config[\"input_width\"])\n                )\n            )\n            .squeeze()\n            .shape\n        )\n        input_fc = int(height * width * nb_channels)\n        # fully connected linear layers\n        self.linear_layers = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(in_features=input_fc, out_features=512),\n            nn.ReLU(),\n            nn.Dropout2d(0.5),\n            nn.Linear(in_features=512, out_features=512),\n            nn.ReLU(),\n            nn.Dropout2d(0.5),\n            nn.Linear(in_features=512, out_features=28),\n        )\n\n    def forward(self, x):\n        x = self.layers(x)\n#         x = self.max_pool(x)\n        x = self.linear_layers(x)\n        return x\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        metrics = {name: metric(y, y_hat) for name, metric in self.metrics.items()}\n        self.log_dict(metrics, on_step=False, on_epoch=True, prog_bar=True)\n        return metrics\n\n    def configure_metrics(self):\n        self.metrics = {\n            \"val_loss\": self.criterion.to(self.config[\"device\"]),\n            \"mae\": torchmetrics.MeanAbsoluteError().to(self.config[\"device\"]),\n            \"mape\": torchmetrics.MeanAbsolutePercentageError().to(\n                self.config[\"device\"]\n            ),\n            \"smape\": torchmetrics.SymmetricMeanAbsolutePercentageError().to(\n                self.config[\"device\"]\n            ),\n            \"r2_score\": torchmetrics.R2Score(num_outputs=28).to(self.config[\"device\"]),\n            \"cosine_similarity\": torchmetrics.CosineSimilarity(reduction=\"mean\").to(\n                self.config[\"device\"]\n            ),\n        }\n    \n    \n\nconfig[\"total_layers\"] = 44\nconfig[\"fixed_layers\"] = 0\nmodel = PreTrainedVGG(config)\nprint(model)\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"[INFO]: {total_params:,} total parameters.\")\nmodel(torch.rand((1, 3, config[\"input_width\"], config[\"nb_image_per_axis\"]*config[\"input_width\"])))\n","metadata":{"execution":{"iopub.status.busy":"2022-04-14T06:50:08.35156Z","iopub.execute_input":"2022-04-14T06:50:08.351957Z","iopub.status.idle":"2022-04-14T06:50:38.221594Z","shell.execute_reply.started":"2022-04-14T06:50:08.351917Z","shell.execute_reply":"2022-04-14T06:50:38.220854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checkpoint\n\nWe had 2 checkpoints to our training:\n\n* one for saving our model every time we have a minimum in the validation loss \n* one for saving the model's and data module script","metadata":{}},{"cell_type":"code","source":"model_checkpoint = pl.callbacks.model_checkpoint.ModelCheckpoint(\n    dirpath=run.dir if \"run\" in locals() else \"tmp/\",\n    filename=\"{epoch}-{val_loss:.3f}\",\n    monitor=\"val_loss\",\n    mode=\"min\",\n    verbose=True,\n    save_last=True,\n)\n\nscript_checkpoint = training.ScriptCheckpoint(\n    dirpath=run.dir if \"run\" in locals() else \"tmp/\",\n)\n\ncallbacks = [script_checkpoint]\nlog = None\nif config[\"job_type\"] == \"train\" or True:\n    callbacks.append(model_checkpoint)\n    print(f\"[INFO]: saving models.\")\nelse:\n    print(f\"[INFO]: not saving models.\")\nif config[\"job_type\"] == \"debug\":\n    log = \"all\"\n","metadata":{"execution":{"iopub.status.busy":"2022-04-14T06:50:38.222684Z","iopub.execute_input":"2022-04-14T06:50:38.222871Z","iopub.status.idle":"2022-04-14T06:50:38.233208Z","shell.execute_reply.started":"2022-04-14T06:50:38.222848Z","shell.execute_reply":"2022-04-14T06:50:38.232455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training\n\nWe then train our model.","metadata":{}},{"cell_type":"code","source":"if config[\"log_wandb\"]:\n    wandb_logger = pl.loggers.WandbLogger()\n    wandb_logger.watch(model, log=log, log_graph=True)\nelse:\n    wandb_logger = None\ntrainer = pl.Trainer(\n    max_epochs=700,\n    max_time={\"hours\": 11},\n    callbacks=callbacks,\n    logger=wandb_logger,\n    devices=\"auto\",\n    accelerator=\"auto\",\n#     limit_train_batches=0.3,\n#     limit_val_batches=1,\n#     log_every_n_steps=1,\n)\ntrainer.fit(\n    model,\n    datamodule=dm,\n)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-14T06:50:38.234732Z","iopub.execute_input":"2022-04-14T06:50:38.235168Z","iopub.status.idle":"2022-04-14T10:37:52.342104Z","shell.execute_reply.started":"2022-04-14T06:50:38.235128Z","shell.execute_reply":"2022-04-14T10:37:52.341353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dm.prepare_data()\ndm.setup(\"validate\")\npredictions = torch.cat(trainer.predict(model, dataloaders=dm.predict_dataloader()))\ntargets = torch.FloatTensor(dm.targets)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T10:37:52.343959Z","iopub.execute_input":"2022-04-14T10:37:52.34422Z","iopub.status.idle":"2022-04-14T10:37:59.852241Z","shell.execute_reply.started":"2022-04-14T10:37:52.344184Z","shell.execute_reply":"2022-04-14T10:37:59.851367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_output = training.SaveOutput()\nhandle = model.layers[3].register_forward_hook(save_output)\nimage = images[0]\nmodel(image.unsqueeze(0))\nhandle.remove()\noutputs = save_output.outputs[0].permute(1, 0, 2, 3).detach().cpu()[:30]\ngrid_img = utils.make_grid(outputs, normalize=True, pad_value=1, padding=1)\nplt.figure(figsize=(30, 30))\nplt.imshow(grid_img.permute(1, 2, 0))","metadata":{"execution":{"iopub.status.busy":"2022-04-14T10:37:59.855282Z","iopub.execute_input":"2022-04-14T10:37:59.855581Z","iopub.status.idle":"2022-04-14T10:38:01.817755Z","shell.execute_reply.started":"2022-04-14T10:37:59.855542Z","shell.execute_reply":"2022-04-14T10:38:01.817129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plotting.plot_kde(\n    [dm.scaler.transform(targets.cpu().numpy()),\n    predictions.cpu().numpy()],\n    nb_hist_per_line=6,\n    columns=dm.train_df.columns[1:-1],\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T10:38:01.819206Z","iopub.execute_input":"2022-04-14T10:38:01.819933Z","iopub.status.idle":"2022-04-14T10:38:07.20148Z","shell.execute_reply.started":"2022-04-14T10:38:01.819895Z","shell.execute_reply":"2022-04-14T10:38:07.200759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run.finish()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T10:38:07.202744Z","iopub.execute_input":"2022-04-14T10:38:07.20313Z","iopub.status.idle":"2022-04-14T10:38:22.745019Z","shell.execute_reply.started":"2022-04-14T10:38:07.203093Z","shell.execute_reply":"2022-04-14T10:38:22.744292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}