{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Network per channel\n","\n","As we dive more precisely into the topic, we now create a more robust dataset:\n","\n","* each input is an image of size `(C, W, H)`. On each channel, there is a grayscale sliced image. So, \n","  * the first `C/3` channels are sliced images along x axis\n","  * the following `C/3` channels are sliced images along x axis\n","  * the last `C/3` channels are sliced images along x axis\n","* each output is list of fabric descriptors\n","\n","If we take `C=3`, we can use a pretrained VGG model. Indeed, this model is trained on RGB images, which have 3 channels."]},{"cell_type":"markdown","metadata":{},"source":["# Importing the dataframe"]},{"cell_type":"markdown","metadata":{},"source":["Firstly, we initialize wandb. It is a tool that allows to store the losses and retrieve the deframe. Otherwise, you can directly access locally the dataframe on your computer."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-01T10:50:50.260671Z","iopub.status.busy":"2022-04-01T10:50:50.260295Z","iopub.status.idle":"2022-04-01T10:51:03.299346Z","shell.execute_reply":"2022-04-01T10:51:03.298537Z","shell.execute_reply.started":"2022-04-01T10:50:50.260558Z"},"executionInfo":{"elapsed":4758,"status":"ok","timestamp":1644495641039,"user":{"displayName":"Matias Etcheverry","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAaRgyKc_t8sZam6khtkO8IvEB4xqvz8dwKcJh=s64","userId":"13887759295001408411"},"user_tz":-60},"id":"KCF0gEhbI9yg","outputId":"65a442db-4731-422d-a7a4-ae8f237f375d","trusted":true},"outputs":[],"source":["!pip install wandb --upgrade"]},{"cell_type":"markdown","metadata":{},"source":["We import all the useful packages."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-01T10:51:03.301761Z","iopub.status.busy":"2022-04-01T10:51:03.301471Z","iopub.status.idle":"2022-04-01T10:51:08.136038Z","shell.execute_reply":"2022-04-01T10:51:08.135215Z","shell.execute_reply.started":"2022-04-01T10:51:03.301727Z"},"executionInfo":{"elapsed":12323,"status":"ok","timestamp":1644495653358,"user":{"displayName":"Matias Etcheverry","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAaRgyKc_t8sZam6khtkO8IvEB4xqvz8dwKcJh=s64","userId":"13887759295001408411"},"user_tz":-60},"id":"396f8318","outputId":"ecbf203b-7c9f-4285-a787-3c279f8be90f","trusted":true},"outputs":[],"source":["import sys\n","from pathlib import Path\n","\n","IS_COLAB = \"google.colab\" in sys.modules\n","IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n","if IS_KAGGLE:\n","    repo_path = Path(\"../input/microstructure-reconstruction\")\n","elif IS_COLAB:\n","    from google.colab import drive\n","\n","    drive.mount(\"/content/gdrive\")\n","    repo_path = Path(\"/content/gdrive/MyDrive/microstructure-reconstruction\")\n","else:\n","    repo_path = Path(\"/home/matias/microstructure-reconstruction\")\n","sys.path.append(str(repo_path))\n","\n","from copy import deepcopy\n","from importlib import reload\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from PIL import Image\n","import pandas as pd\n","import pytorch_lightning as pl\n","import torch\n","from typing import Union, List\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchmetrics\n","import torchvision.models as pretrained_models\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KernelDensity\n","from sklearn.preprocessing import MinMaxScaler\n","from torch.utils.data import DataLoader\n","from torchvision import transforms, utils\n","from tqdm import tqdm\n","\n","import wandb\n","from custom_datasets import dataset\n","from custom_models import models\n","from tools import dataframe_reformat, inspect_code, plotting, training, wandb_api\n","\n","log_wandb = True\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","kwargs = {\"num_workers\": 2, \"pin_memory\": True} if use_cuda else {\"num_workers\": 4}\n","print(f\"[INFO]: Computation device: {device}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["We initialize a wandb run, that will save our metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-01T10:51:08.137559Z","iopub.status.busy":"2022-04-01T10:51:08.137288Z","iopub.status.idle":"2022-04-01T10:51:16.953175Z","shell.execute_reply":"2022-04-01T10:51:16.952506Z","shell.execute_reply.started":"2022-04-01T10:51:08.137506Z"},"executionInfo":{"elapsed":6825,"status":"ok","timestamp":1644495660175,"user":{"displayName":"Matias Etcheverry","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAaRgyKc_t8sZam6khtkO8IvEB4xqvz8dwKcJh=s64","userId":"13887759295001408411"},"user_tz":-60},"id":"dj85A2mMIz_j","outputId":"91b1c0e2-dbca-4381-e5ac-9f404baf57db","trusted":true},"outputs":[],"source":["if log_wandb:\n","    import wandb\n","\n","    wandb_api.login()\n","    run = wandb.init(\n","        project=\"microstructure-reconstruction\",\n","        group=\"NChannel Network\",\n","        job_type=\"test\",\n","    )\n"]},{"cell_type":"markdown","metadata":{},"source":["Parameters of our run:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-01T10:51:16.956063Z","iopub.status.busy":"2022-04-01T10:51:16.955799Z","iopub.status.idle":"2022-04-01T10:51:16.978659Z","shell.execute_reply":"2022-04-01T10:51:16.977886Z","shell.execute_reply.started":"2022-04-01T10:51:16.956038Z"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1644495660176,"user":{"displayName":"Matias Etcheverry","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAaRgyKc_t8sZam6khtkO8IvEB4xqvz8dwKcJh=s64","userId":"13887759295001408411"},"user_tz":-60},"id":"w2Me9kwpIz_j","outputId":"f4290858-5c72-4331-ed8d-b99ffeb3b058","trusted":true},"outputs":[],"source":["if log_wandb:\n","    config = wandb.config\n","else:\n","    config = wandb_api.Config()\n","\n","config[\"job_type\"] = run.job_type\n","config[\"train_val_split\"] = 0.7\n","config[\"seed\"] = 42\n","config[\"batch_size\"] = 64\n","config[\"learning_rate\"] = 0.0001\n","config[\"device\"] = device\n","config[\"momentum\"] = 0.9\n","config[\"architecture\"] = \"pretrained VGG\"\n","config[\"input_width\"] = 64\n","config[\"weight_decay\"] = 0.00005\n","config[\"epochs\"] = 0\n","config[\"frac_sample\"] = 1\n","config[\"frac_noise\"] = 0.1\n","config[\"nb_image_per_axis\"] = 3\n","config[\"log_wandb\"] = log_wandb\n","torch.manual_seed(config[\"seed\"])\n","pl.seed_everything(config[\"seed\"])"]},{"cell_type":"markdown","metadata":{},"source":["We retrieve the dataframe containing the descriptors. This can locally be done on your computer."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-01T10:51:16.980413Z","iopub.status.busy":"2022-04-01T10:51:16.980158Z","iopub.status.idle":"2022-04-01T10:51:18.160454Z","shell.execute_reply":"2022-04-01T10:51:18.159738Z","shell.execute_reply.started":"2022-04-01T10:51:16.980379Z"},"trusted":true},"outputs":[],"source":["class DataModule(pl.LightningDataModule):\n","    def __init__(\n","        self,\n","        config,\n","        repo_path,\n","        train_df=None,\n","        test_df=None,\n","        train_dataset=None,\n","        validation_dataset=None,\n","    ):\n","        super().__init__()\n","        self.config = config\n","        self.repo_path = repo_path\n","        self.train_df = train_df.convert_dtypes() if train_df is not None else None\n","        self.test_df = test_df.convert_dtypes() if test_df is not None else None\n","        self.train_dataset = train_dataset\n","        self.validation_dataset = validation_dataset\n","\n","        if self.config[\"log_wandb\"]:\n","            if self.train_df is None:\n","                self.training_data_at = wandb.Api().artifact(\n","                    \"matiasetcheverry/microstructure-reconstruction/train_df:10_images\"\n","                )\n","            if self.test_df is None:\n","                self.test_data_at = wandb.Api().artifact(\n","                    \"matiasetcheverry/microstructure-reconstruction/test_df:10_images\"\n","                )\n","\n","        self.transform = transforms.Compose(\n","            [\n","                transforms.GaussianBlur(kernel_size=3, sigma=0.5),\n","#                 transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","            ]\n","        )\n","\n","    def prepare_data(self):\n","        if self.config[\"log_wandb\"]:\n","            if self.train_df is None:\n","                self.training_data_at.download()\n","            if self.test_df is None:\n","                self.test_data_at.download()\n","\n","    def _init_df_wandb(self):\n","        if self.train_df is None:\n","            self.train_df = wandb_api.convert_table_to_dataframe(\n","                self.training_data_at.get(\"fabrics\")\n","            )\n","            self.train_df[\"photos\"] = self.train_df[\"photos\"].apply(\n","                func=lambda photo_paths: [\n","                    str(self.repo_path / Path(x)) for x in photo_paths\n","                ]\n","            )\n","        if self.test_df is None:\n","            self.test_df = wandb_api.convert_table_to_dataframe(\n","                self.test_data_at.get(\"fabrics\")\n","            )\n","            self.test_df[\"photos\"] = self.test_df[\"photos\"].apply(\n","                func=lambda photo_paths: [\n","                    str(self.repo_path / Path(x)) for x in photo_paths\n","                ]\n","            )\n","\n","    def _init_df_local(self):\n","        fabrics_df = pd.read_csv(self.repo_path / \"REV1_600/fabrics.txt\")\n","        path_to_slices = self.repo_path / \"REV1_600/REV1_600Slices\"\n","        fabrics_df[\"photos\"] = fabrics_df[\"id\"].apply(\n","            func=dataframe_reformat.associate_rev_id_to_its_images,\n","            args=(path_to_slices, 10),\n","        )\n","        fabrics_df = fabrics_df[fabrics_df.photos.str.len().gt(0)]\n","        fabrics_df[\"photos\"] = fabrics_df[\"photos\"].apply(func=lambda x: sorted(x))\n","        train_df, test_df = train_test_split(\n","            fabrics_df,\n","            train_size=config[\"train_val_split\"],\n","            random_state=config[\"seed\"],\n","            shuffle=True,\n","        )\n","        if self.train_df is None:\n","            self.train_df = train_df.reset_index(drop=True)\n","        if self.test_df is None:\n","            self.test_df = test_df.reset_index(drop=True)\n","\n","    def init_df(self):\n","        if self.config[\"log_wandb\"]:\n","            self._init_df_wandb()\n","        else:\n","            self._init_df_local()\n","\n","    def setup(self, stage):\n","        if self.train_dataset is None or self.validation_dataset is None:\n","            self.init_df()\n","            self.scaler = MinMaxScaler(feature_range=(0, 1))\n","            self.scaler.partial_fit(self.train_df.iloc[:, 1:-1])\n","            self.scaler.partial_fit(self.test_df.iloc[:, 1:-1])\n","            \n","            normalized_train_df = deepcopy(self.train_df)\n","            normalized_train_df.iloc[:, 1:-1] = self.scaler.transform(\n","                self.train_df.iloc[:, 1:-1]\n","            )\n","            normalized_test_df = deepcopy(self.test_df)\n","            normalized_test_df.iloc[:, 1:-1] = self.scaler.transform(\n","                self.test_df.iloc[:, 1:-1]\n","            )\n","            self.kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.75).fit(\n","                normalized_train_df.iloc[:, 1:-1].to_numpy()\n","            )\n","            if self.train_dataset is None:\n","                self.train_dataset = dataset.NChannelPhotosDataset(\n","                    normalized_train_df,\n","                    nb_input_photos_per_plane=self.config[\"nb_image_per_axis\"],\n","                    width=self.config[\"input_width\"],\n","                    transform=self.transform,\n","                    proba_rotating=0,\n","                    noise=self.config[\"frac_noise\"],\n","                )\n","            if self.validation_dataset is None:\n","                self.validation_dataset = dataset.NChannelPhotosDataset(\n","                    normalized_test_df,\n","                    nb_input_photos_per_plane=self.config[\"nb_image_per_axis\"],\n","                    width=self.config[\"input_width\"],\n","                    transform=self.transform,\n","                    proba_rotating=0,\n","                    noise=0,\n","                )\n","            self.targets = self.test_df.iloc[:, 1:-1].to_numpy()\n","\n","    def train_dataloader(self):\n","        return DataLoader(\n","            self.train_dataset,\n","            batch_size=self.config[\"batch_size\"],\n","            shuffle=True,\n","            **kwargs,\n","        )\n","\n","    def val_dataloader(self):\n","        return DataLoader(\n","            self.validation_dataset,\n","            batch_size=self.config[\"batch_size\"],\n","            shuffle=False,\n","            **kwargs,\n","        )\n","\n","    def test_dataloader(self):\n","        return self.val_dataloader()\n","\n","    def predict_dataloader(self):\n","        return DataLoader(\n","            [image for image, _ in self.validation_dataset],\n","            batch_size=self.config[\"batch_size\"],\n","            shuffle=False,\n","            **kwargs,\n","        )\n","\n","dm = DataModule(config, repo_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-01T10:51:18.162075Z","iopub.status.busy":"2022-04-01T10:51:18.161832Z","iopub.status.idle":"2022-04-01T10:51:37.331416Z","shell.execute_reply":"2022-04-01T10:51:37.330582Z","shell.execute_reply.started":"2022-04-01T10:51:18.162044Z"},"trusted":true},"outputs":[],"source":["dm.prepare_data()\n","dm.setup(stage=\"fit\")\n","first_batch = next(iter(dm.train_dataloader()))\n","print(\"Nb of descriptors:\", len(first_batch[1][0]))\n","print(\"Nb batch in dataset:\", len(dm.train_dataloader()))\n","print(\"Size of a batch:\", len(first_batch[0]))\n","images, labels = first_batch[0], first_batch[1]\n","print(\"Image shape:\", images[0].shape)\n","# grid = utils.make_grid(images)\n","# fig = plt.figure(figsize=(40, 10))\n","# plt.imshow(grid.numpy().transpose((1, 2, 0)))\n","# plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["# Network definition"]},{"cell_type":"markdown","metadata":{},"source":["The next step is to define our model. This model is inspired by VGG11:\n","\n","* we define several convulational blocks.\n","* each of this block is sequence of:\n","  * convulational layer with `kernel_size=3, padding=1`\n","  * activation function, here it is the `ReLU`\n","  * max pooling layer with `kernel_size=2, stride=2` which aims at reducing the size of the convolutional layers"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-01T10:51:37.334114Z","iopub.status.busy":"2022-04-01T10:51:37.333539Z","iopub.status.idle":"2022-04-01T10:52:00.373751Z","shell.execute_reply":"2022-04-01T10:52:00.372939Z","shell.execute_reply.started":"2022-04-01T10:51:37.334069Z"},"trusted":true},"outputs":[],"source":["class PreTrainedVGG(models.BaseModel):\n","    def __init__(self, config, scaler=None):\n","        super().__init__(config)\n","\n","        self.config = config\n","        self.config[\"model_type\"] = type(self)\n","        self.scaler = scaler\n","\n","        self.configure_model()\n","        self.configure_criterion()\n","        self.configure_metrics()\n","\n","    def configure_model(self):      \n","        assert self.config[\"total_layers\"] >= self.config[\"fixed_layers\"]\n","        vgg = pretrained_models.vgg16_bn(pretrained=True)\n","        vgg = list(vgg.features.children())[: self.config[\"total_layers\"]]\n","        vgg[0] = nn.Conv2d(self.config[\"nb_image_per_axis\"]*3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        self.layers = nn.Sequential(*vgg)\n","        for idx, child in enumerate(self.layers.children()):\n","            if idx < self.config[\"fixed_layers\"] and isinstance(child, nn.Conv2d):\n","                for param in child.parameters():\n","                    param.requires_grad = False\n","#             else:\n","#                 reset_parameters = getattr(child, \"reset_parameters\", None)\n","#                 if callable(reset_parameters):\n","#                     child.reset_parameters()\n","        nb_channels, width, a = (\n","            self.layers(\n","                torch.rand(\n","                    (1, self.config[\"nb_image_per_axis\"]*3, self.config[\"input_width\"], self.config[\"input_width\"])\n","                )\n","            )\n","            .squeeze()\n","            .shape\n","        )\n","\n","        input_fc = int(width**2 * nb_channels)\n","        # fully connected linear layers\n","        self.linear_layers = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(in_features=input_fc, out_features=512),\n","            nn.ReLU(),\n","            nn.Dropout2d(0.5),\n","            nn.Linear(in_features=512, out_features=512),\n","            nn.ReLU(),\n","            nn.Dropout2d(0.5),\n","            nn.Linear(in_features=512, out_features=23),\n","        )\n","\n","    def forward(self, x):\n","        x = self.layers(x)\n","        x = self.linear_layers(x)\n","        return x\n","    \n","    def training_step(self, batch, batch_idx):\n","        x, y = batch\n","        y_hat = self(x)\n","        loss = self.criterion(y_hat.float(), y.float())\n","        self.log(\n","            \"train_loss\",\n","            loss,\n","            on_step=False,\n","            on_epoch=True,\n","        )\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y = batch\n","        y_hat = self(x)\n","        metrics = {name: metric(y, y_hat) for name, metric in self.metrics.items()}\n","        self.log_dict(metrics, on_step=False, on_epoch=True, prog_bar=True)\n","        return metrics\n","    \n","    def configure_criterion(self):\n","        self.criterion = nn.L1Loss()\n","        self.config[\"loss_type\"] = type(self.criterion)\n","\n","config[\"total_layers\"] = 44\n","config[\"fixed_layers\"] = 0\n","model = PreTrainedVGG(config)\n","print(model)\n","total_params = sum(p.numel() for p in model.parameters())\n","print(f\"[INFO]: {total_params:,} total parameters.\")\n","model(torch.rand((1, config[\"nb_image_per_axis\"]*3, config[\"input_width\"], config[\"input_width\"])))\n"]},{"cell_type":"markdown","metadata":{},"source":["# Checkpoint\n","\n","We had 2 checkpoints to our training:\n","\n","* one for saving our model every time we have a minimum in the validation loss \n","* one for saving the model's and data module script"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-01T10:52:00.375831Z","iopub.status.busy":"2022-04-01T10:52:00.375294Z","iopub.status.idle":"2022-04-01T10:52:00.386307Z","shell.execute_reply":"2022-04-01T10:52:00.38519Z","shell.execute_reply.started":"2022-04-01T10:52:00.375788Z"},"trusted":true},"outputs":[],"source":["model_checkpoint = pl.callbacks.model_checkpoint.ModelCheckpoint(\n","    dirpath=run.dir if \"run\" in locals() else \"tmp/\",\n","    filename=\"{epoch}-{val_loss:.3f}\",\n","    monitor=\"val_loss\",\n","    mode=\"min\",\n","    verbose=True,\n","    save_last=True,\n",")\n","\n","script_checkpoint = training.ScriptCheckpoint(\n","    dirpath=run.dir if \"run\" in locals() else \"tmp/\",\n",")\n","\n","callbacks = [script_checkpoint]\n","log = None\n","if config[\"job_type\"] == \"train\" or True:\n","    callbacks.append(model_checkpoint)\n","    print(f\"[INFO]: saving models.\")\n","else:\n","    print(f\"[INFO]: not saving models.\")\n","if config[\"job_type\"] == \"debug\":\n","    log = \"all\"\n"]},{"cell_type":"markdown","metadata":{},"source":["# Training\n","\n","We then train our model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-01T10:52:00.388381Z","iopub.status.busy":"2022-04-01T10:52:00.387971Z","iopub.status.idle":"2022-04-01T13:02:08.627198Z","shell.execute_reply":"2022-04-01T13:02:08.626424Z","shell.execute_reply.started":"2022-04-01T10:52:00.38834Z"},"trusted":true},"outputs":[],"source":["if config[\"log_wandb\"]:\n","    wandb_logger = pl.loggers.WandbLogger()\n","    wandb_logger.watch(model, log=log, log_graph=True)\n","else:\n","    wandb_logger = None\n","trainer = pl.Trainer(\n","    max_epochs=155,\n","    callbacks=callbacks,\n","    logger=wandb_logger,\n","    devices=\"auto\",\n","    accelerator=\"auto\",\n","#     limit_train_batches=0.3,\n","#     limit_val_batches=1,\n","#     log_every_n_steps=1,\n",")\n","trainer.fit(\n","    model,\n","    datamodule=dm,\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-01T13:02:08.630582Z","iopub.status.busy":"2022-04-01T13:02:08.630297Z","iopub.status.idle":"2022-04-01T13:02:28.899078Z","shell.execute_reply":"2022-04-01T13:02:28.898253Z","shell.execute_reply.started":"2022-04-01T13:02:08.630544Z"},"trusted":true},"outputs":[],"source":["dm.prepare_data()\n","dm.setup(\"validate\")\n","predictions = torch.cat(trainer.predict(model, dataloaders=dm.predict_dataloader()))\n","targets = torch.FloatTensor(dm.targets)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-01T13:02:28.901055Z","iopub.status.busy":"2022-04-01T13:02:28.9008Z","iopub.status.idle":"2022-04-01T13:02:29.770143Z","shell.execute_reply":"2022-04-01T13:02:29.769379Z","shell.execute_reply.started":"2022-04-01T13:02:28.90102Z"},"trusted":true},"outputs":[],"source":["save_output = training.SaveOutput()\n","handle = model.layers[3].register_forward_hook(save_output)\n","image = images[0]\n","model(image.unsqueeze(0))\n","handle.remove()\n","outputs = save_output.outputs[0].permute(1, 0, 2, 3).detach().cpu()[:30]\n","grid_img = utils.make_grid(outputs, normalize=True, pad_value=1, padding=1)\n","plt.figure(figsize=(30, 30))\n","plt.imshow(grid_img.permute(1, 2, 0))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-01T13:02:29.77139Z","iopub.status.busy":"2022-04-01T13:02:29.771153Z","iopub.status.idle":"2022-04-01T13:02:34.430181Z","shell.execute_reply":"2022-04-01T13:02:34.429513Z","shell.execute_reply.started":"2022-04-01T13:02:29.77136Z"},"trusted":true},"outputs":[],"source":["fig = plotting.plot_kde(\n","    [targets.cpu().numpy(),\n","    dm.scaler.inverse_transform(predictions.cpu().numpy())],\n","    nb_hist_per_line=6,\n","    columns=dm.train_df.columns[1:-1],\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-01T13:02:34.432057Z","iopub.status.busy":"2022-04-01T13:02:34.431577Z","iopub.status.idle":"2022-04-01T13:02:44.056137Z","shell.execute_reply":"2022-04-01T13:02:44.055379Z","shell.execute_reply.started":"2022-04-01T13:02:34.432017Z"},"trusted":true},"outputs":[],"source":["run.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
