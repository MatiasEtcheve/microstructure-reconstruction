{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Simple CNN network\n\nThe original problem is:\n\n    (P) Compute the fabrics from multiple sliced images per rev\n\nWe are firstly having an intermediate objective:\n\n    (P') Compute the fabrics from one single sliced images per rev\n\nOf course, the accuracy will be much worse because the fabrics are computed in every direction. However, it is a good try.","metadata":{}},{"cell_type":"markdown","source":"# Importing the dataframe\n\nFirstly, we initialize wandb. It is a tool that allows to store the losses and retrieve the deframe. Otherwise, you can directly access locally the dataframe on your computer.","metadata":{"execution":{"iopub.status.busy":"2022-02-18T18:22:35.714432Z","iopub.execute_input":"2022-02-18T18:22:35.71475Z","iopub.status.idle":"2022-02-18T18:22:35.719176Z","shell.execute_reply.started":"2022-02-18T18:22:35.714718Z","shell.execute_reply":"2022-02-18T18:22:35.718188Z"}}},{"cell_type":"code","source":"!pip install wandb --upgrade","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We import all the useful packages.","metadata":{}},{"cell_type":"code","source":"import sys\nfrom pathlib import Path\n\nIS_COLAB = \"google.colab\" in sys.modules\nIS_KAGGLE = \"kaggle_secrets\" in sys.modules\nif IS_KAGGLE:\n    repo_path = Path(\"../input/microstructure-reconstruction\")\nelif IS_COLAB:\n    from google.colab import drive\n\n    drive.mount(\"/content/gdrive\")\n    repo_path = Path(\"/content/gdrive/MyDrive/microstructure-reconstruction\")\nelse:\n    repo_path = Path(\"/home/matias/microstructure-reconstruction\")\nsys.path.append(str(repo_path))\n\nfrom copy import deepcopy\nfrom importlib import reload\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\nimport pandas as pd\nimport pytorch_lightning as pl\nimport torch\nfrom typing import Union, List\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchmetrics\nimport torchvision.models as pretrained_models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KernelDensity\nfrom sklearn.preprocessing import MinMaxScaler\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms, utils\nfrom tqdm import tqdm\n\nimport wandb\nfrom custom_datasets import dataset\nfrom custom_models import models\nfrom tools import dataframe_reformat, inspect_code, plotting, training, wandb_api\n\nlog_wandb = True\n\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\nkwargs = {\"num_workers\": 2, \"pin_memory\": True} if use_cuda else {\"num_workers\": 4}\nprint(f\"[INFO]: Computation device: {device}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We initialize a wandb run, that will save our metrics","metadata":{}},{"cell_type":"code","source":"if log_wandb:\n    import wandb\n    wandb_api.login()\n    run = wandb.init(\n        project=\"microstructure-reconstruction\",\n        group=\"Naive Network\",\n        job_type=\"test\",\n    )\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Parameters of our run:","metadata":{}},{"cell_type":"code","source":"if log_wandb:\n    config = wandb.config\nelse:\n    config = wandb_api.Config()\n\nconfig[\"job_type\"] = run.job_type\nconfig[\"train_val_split\"] = 0.7\nconfig[\"seed\"] = 42\nconfig[\"batch_size\"] = 32\nconfig[\"learning_rate\"] = 0.0003\nconfig[\"device\"] = device\nconfig[\"momentum\"] = 0.7\nconfig[\"architecture\"] = \"VGG\"\nconfig[\"input_width\"] = 100\nconfig[\"weight_decay\"] = 0.00005\nconfig[\"epochs\"] = 0\nconfig[\"frac_sample\"] = 1\nconfig[\"frac_noise\"] = 0.9\n# config[\"total_layers\"] = 24\n# config[\"fixed_layers\"] = 0\nconfig[\"log_wandb\"] = log_wandb\ntorch.manual_seed(config[\"seed\"])\npl.seed_everything(config[\"seed\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataModule(pl.LightningDataModule):\n    def __init__(\n        self,\n        config,\n        repo_path,\n        train_df=None,\n        test_df=None,\n    ):\n        super().__init__()\n        self.config = config\n        self.repo_path = repo_path\n        self.train_df = train_df.convert_dtypes() if train_df is not None else None\n        self.test_df = test_df.convert_dtypes() if test_df is not None else None\n\n        if self.config[\"log_wandb\"]:\n            if self.train_df is None:\n                self.training_data_at = wandb.Api().artifact(\n                    \"matiasetcheverry/microstructure-reconstruction/train_df:10_images\"\n                )\n            if self.test_df is None:\n                self.test_data_at = wandb.Api().artifact(\n                    \"matiasetcheverry/microstructure-reconstruction/test_df:10_images\"\n                )\n\n        self.transform = transforms.Compose(\n            [\n                transforms.CenterCrop(207),\n                transforms.Resize(\n                    (self.config[\"input_width\"], self.config[\"input_width\"])\n                ),\n                transforms.ToTensor(),\n                transforms.GaussianBlur(kernel_size=3, sigma=0.5),\n            ]\n        )\n\n    def prepare_data(self):\n        if self.config[\"log_wandb\"]:\n            if self.train_df is None:\n                self.training_data_at.download()\n            if self.test_df is None:\n                self.test_data_at.download()\n\n    def _init_df_wandb(self):\n        if self.train_df is None:\n            self.train_df = wandb_api.convert_table_to_dataframe(\n                self.training_data_at.get(\"fabrics\")\n            )\n            self.train_df[\"photos\"] = self.train_df[\"photos\"].apply(\n                func=lambda photo_paths: [\n                    str(self.repo_path / Path(x)) for x in photo_paths\n                ]\n            )\n        if self.test_df is None:\n            self.test_df = wandb_api.convert_table_to_dataframe(\n                self.test_data_at.get(\"fabrics\")\n            )\n            self.test_df[\"photos\"] = self.test_df[\"photos\"].apply(\n                func=lambda photo_paths: [\n                    str(self.repo_path / Path(x)) for x in photo_paths\n                ]\n            )\n\n    def _init_df_local(self):\n        fabrics_df = pd.read_csv(self.repo_path / \"REV1_600/fabrics.txt\")\n        path_to_slices = self.repo_path / \"REV1_600/REV1_600Slices\"\n        fabrics_df[\"photos\"] = fabrics_df[\"id\"].apply(\n            func=dataframe_reformat.associate_rev_id_to_its_images,\n            args=(path_to_slices, 10),\n        )\n        fabrics_df = fabrics_df[fabrics_df.photos.str.len().gt(0)]\n        fabrics_df[\"photos\"] = fabrics_df[\"photos\"].apply(func=lambda x: sorted(x))\n        train_df, test_df = train_test_split(\n            fabrics_df,\n            train_size=config[\"train_val_split\"],\n            random_state=config[\"seed\"],\n            shuffle=True,\n        )\n        if self.train_df is None:\n            self.train_df = train_df.reset_index(drop=True)\n        if self.test_df is None:\n            self.test_df = test_df.reset_index(drop=True)\n\n    def init_df(self):\n        if self.config[\"log_wandb\"]:\n            self._init_df_wandb()\n        else:\n            self._init_df_local()\n\n    def setup(self, stage):\n        self.init_df()\n        self.scaler = MinMaxScaler(feature_range=(0, 1))\n        self.scaler.fit_transform(self.train_df.iloc[:, 1:-1])\n        self.scaler.transform(self.test_df.iloc[:, 1:-1])\n        normalized_train_df = deepcopy(self.train_df)\n        normalized_train_df.iloc[:, 1:-1] = self.scaler.transform(\n            self.train_df.iloc[:, 1:-1]\n        )\n        normalized_test_df = deepcopy(self.test_df)\n        normalized_test_df.iloc[:, 1:-1] = self.scaler.transform(\n            self.test_df.iloc[:, 1:-1]\n        )\n        self.kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.75).fit(\n            normalized_train_df.iloc[:, 1:-1].to_numpy()\n        )\n\n        self.train_dataset = dataset.SinglePhotoDataset(\n            normalized_train_df, transform=self.transform, noise=config[\"frac_noise\"]\n        )\n        self.validation_dataset = dataset.SinglePhotoDataset(\n            normalized_test_df, transform=self.transform, noise=0\n        )\n        self.targets = self.test_df.iloc[:, 1:-1].to_numpy()\n\n        \n\n    def train_dataloader(self):\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.config[\"batch_size\"],\n            shuffle=True,\n            **kwargs,\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.validation_dataset,\n            batch_size=self.config[\"batch_size\"],\n            shuffle=False,\n            **kwargs,\n        )\n\n    def test_dataloader(self):\n        return self.val_dataloader()\n\n    def predict_dataloader(self):\n        return DataLoader(\n            [image for image, _ in self.validation_dataset],\n            batch_size=self.config[\"batch_size\"],\n            shuffle=False,\n            **kwargs,\n        )\n\n\ndm = DataModule(config, repo_path)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dm.prepare_data()\ndm.setup(stage=\"fit\")\nfirst_batch = next(iter(dm.train_dataloader()))\n\nimages, labels = first_batch[0], first_batch[1]\ngrid = utils.make_grid(images)\nfig = plt.figure(figsize=(40, 10))\nplt.imshow(grid.numpy().transpose((1, 2, 0)))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VGG11(models.BaseModel):\n    def __init__(self, config, kde = None, scaler=None):\n        super().__init__(config)\n\n        self.config = config\n        self.config[\"model_type\"] = type(self)\n        self.kde = kde\n        self.scaler = scaler\n\n        self.configure_model()\n        self.configure_criterion()\n        self.configure_metrics()\n        \n    def configure_model(self):\n        layers = np.array([64, 128, 256, 512]) // 4\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(1, layers[0], kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(layers[0], layers[1], kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(layers[1], layers[2], kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(layers[2], layers[2], kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(layers[2], layers[3], kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(layers[3], layers[3], kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(layers[3], layers[3], kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(layers[3], layers[3], kernel_size=3, padding=1),\n            nn.BatchNorm2d(layers[3]),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n#             nn.MaxPool2d(kernel_size=int(self.config[\"input_width\"] / (2 ** 4)), stride=int(self.config[\"input_width\"] / (2 ** 4))),\n        )\n        input_fc = int((self.config[\"input_width\"] / (2 ** 5)) ** 2 * layers[3])\n        # fully connected linear layers\n        n = 256\n        self.linear_layers = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(in_features=input_fc, out_features=n),\n            nn.ReLU(),\n            nn.Dropout2d(0.3),\n            nn.Linear(in_features=n, out_features=1),\n            nn.ReLU(),\n            nn.Dropout2d(0.3),\n            nn.Linear(in_features=n, out_features=1),\n        )\n    \n    def forward(self, x):\n        x = self.conv_layers(x)\n        # flatten to prepare for the fully connected layers\n#         x = x.view(x.size(0), -1)\n        x = self.linear_layers(x)\n        return x\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n#         weights = self.kde.score_samples(y_hat.cpu().detach().numpy())\n        loss = self.criterion(y_hat, y)\n        self.log(\n            \"train_loss\",\n            loss,\n            on_step=False,\n            on_epoch=True,\n        )\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n#         weights = self.kde.score_samples(y_hat.cpu().detach().numpy())\n        metrics = {name: metric(y, y_hat) for name, metric in self.metrics.items()}\n        metrics[\"val_loss\"] = self.criterion(y_hat, y)\n        self.log_dict(metrics, on_step=False, on_epoch=True, prog_bar=True)\n        return metrics\n\n    def configure_criterion(self):\n        self.criterion = self.L1Loss\n        self.config[\"loss_type\"] = type(self.criterion)\n\n    def L1Loss(self, y, y_hat, weights=None):\n        if weights is not None:\n            weights = nn.functional.normalize(weights, p=1, dim=0)\n            return torch.mean(torch.matmul(weights, torch.abs(y - y_hat)))\n        else: \n            return torch.mean(torch.abs(y - y_hat))\n\n    def configure_metrics(self):\n        self.metrics = {\n            \"mae\": torchmetrics.MeanAbsoluteError().to(self.config[\"device\"]),\n            \"mape\": torchmetrics.MeanAbsolutePercentageError().to(\n                self.config[\"device\"]\n            ),\n            \"smape\": torchmetrics.SymmetricMeanAbsolutePercentageError().to(\n                self.config[\"device\"]\n            ),\n            \"r2_score\": torchmetrics.R2Score(num_outputs=23).to(self.config[\"device\"]),\n            \"cosine_similarity\": torchmetrics.CosineSimilarity(reduction=\"mean\").to(\n                self.config[\"device\"]\n            ),\n        }\n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(\n            self.parameters(),\n            lr=self.config[\"learning_rate\"],\n            weight_decay=self.config[\"weight_decay\"],\n        )\n        self.config[\"optimizer_type\"] = type(optimizer)\n        return optimizer\n\n\nmodel = VGG11(config, kde=dm.kde)\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"[INFO]: {total_params:,} total parameters.\")\nmodel(torch.rand((2, 1, config[\"input_width\"], config[\"input_width\"])))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_checkpoint = pl.callbacks.model_checkpoint.ModelCheckpoint(\n    dirpath=run.dir,\n    filename=\"{epoch}-{val_loss:.3f}\",\n    monitor=\"val_loss\",\n    mode=\"min\",\n    verbose=True,\n    save_last=True,\n)\n\nscript_checkpoint = training.ScriptCheckpoint(\n    dirpath=run.dir,\n)\n\ncallbacks = [script_checkpoint]\nlog = None\nif run.job_type == \"train\" or True:\n    callbacks.append(model_checkpoint)\n    print(f\"[INFO]: saving models.\")\nif run.job_type == \"debug\":\n    log = \"all\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if config[\"log_wandb\"]:\n    wandb_logger = pl.loggers.WandbLogger()\n    wandb_logger.watch(model, log=log, log_graph=True)\nelse:\n    wandb_logger = None\ntrainer = pl.Trainer(\n    max_epochs=150,\n    callbacks=callbacks,\n    logger=wandb_logger,\n    devices=\"auto\", \n    accelerator=\"auto\",\n#     limit_train_batches=0.3, \n#     limit_val_batches=0.3,\n#     log_every_n_steps=1\n)\ntrainer.fit(model, datamodule=dm, )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dm.prepare_data()\ndm.setup(\"validate\")\npredictions = torch.cat(trainer.predict(model, dataloaders=dm.predict_dataloader()))\ntargets = torch.FloatTensor(dm.targets)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_output = training.SaveOutput()\nhandle = model.conv_layers[3].register_forward_hook(save_output)\nimage = images[0]\nmodel(image.unsqueeze(0))\nhandle.remove()\noutputs = save_output.outputs[0].permute(1, 0, 2, 3).detach().cpu()[:30]\ngrid_img = utils.make_grid(outputs, normalize=True, pad_value=1, padding=1)\nplt.figure(figsize=(30, 30))\nplt.imshow(grid_img.permute(1, 2, 0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\n\nfig, ax = plt.subplots()\nsns.kdeplot(\n    data=dm.scaler.transform(targets.cpu().numpy())[:, 4],\n    color=\"orange\",\n    ax=ax,\n    label=\"target\",\n)\nsns.kdeplot(\n    data=predictions.cpu().numpy(),\n    shade=True,\n    color=\"orange\",\n    ax=ax,\n    label=\"predictions\",\n)\nax.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for noise in [0.9, 0.3, 0]:\n    for layer_divider in [4, 16]:\n        for fc_divider in [4, 16]:\n            for dropout in [0.2, 0.6]:\n                if log_wandb:\n                    import wandb\n                    wandb_api.login()\n                    run = wandb.init(\n                        project=\"microstructure-reconstruction\",\n                        group=\"Naive Network\",\n                        job_type=\"test\",\n                    )\n\n                if log_wandb:\n                    config = wandb.config\n                else:\n                    config = wandb_api.Config()\n\n                config[\"job_type\"] = run.job_type\n                config[\"train_val_split\"] = 0.7\n                config[\"seed\"] = 42\n                config[\"batch_size\"] = 32\n                config[\"learning_rate\"] = 0.00006\n                config[\"device\"] = device\n                config[\"momentum\"] = 0.9\n                config[\"architecture\"] = \"VGG\"\n                config[\"input_width\"] = 64\n                config[\"weight_decay\"] = 0.005\n                config[\"epochs\"] = 0\n                config[\"frac_sample\"] = 1\n                config[\"frac_noise\"] = noise\n                # config[\"total_layers\"] = 24\n                # config[\"fixed_layers\"] = 0\n                config[\"log_wandb\"] = log_wandb\n                torch.manual_seed(config[\"seed\"])\n                pl.seed_everything(config[\"seed\"])\n\n                class VGG11(models.BaseModel):\n                    def __init__(self, config, kde = None, scaler=None):\n                        super().__init__(config)\n\n                        self.config = config\n                        self.config[\"model_type\"] = type(self)\n                        self.kde = kde\n                        self.scaler = scaler\n\n                        self.configure_model()\n                        self.configure_criterion()\n                        self.configure_metrics()\n\n                    def configure_model(self):\n                        layers = np.array([64, 128, 256, 512]) // layer_divider\n                        self.conv_layers = nn.Sequential(\n                            nn.Conv2d(1, layers[0], kernel_size=3, padding=1),\n                            nn.ReLU(),\n                            nn.MaxPool2d(kernel_size=2, stride=2),\n                            nn.Conv2d(layers[0], layers[1], kernel_size=3, padding=1),\n                            nn.ReLU(),\n                            nn.MaxPool2d(kernel_size=2, stride=2),\n                            nn.Conv2d(layers[1], layers[2], kernel_size=3, padding=1),\n                            nn.ReLU(),\n                            nn.Conv2d(layers[2], layers[2], kernel_size=3, padding=1),\n                            nn.ReLU(),\n                            nn.MaxPool2d(kernel_size=2, stride=2),\n                            nn.Conv2d(layers[2], layers[3], kernel_size=3, padding=1),\n                            nn.ReLU(),\n                            nn.Conv2d(layers[3], layers[3], kernel_size=3, padding=1),\n                            nn.ReLU(),\n                            nn.MaxPool2d(kernel_size=2, stride=2),\n                            nn.Conv2d(layers[3], layers[3], kernel_size=3, padding=1),\n                            nn.ReLU(),\n                            nn.Conv2d(layers[3], layers[3], kernel_size=3, padding=1),\n                            nn.BatchNorm2d(layers[3]),\n                            nn.ReLU(),\n                            nn.MaxPool2d(kernel_size=2, stride=2)\n                #             nn.MaxPool2d(kernel_size=int(self.config[\"input_width\"] / (2 ** 4)), stride=int(self.config[\"input_width\"] / (2 ** 4))),\n                        )\n                        input_fc = int((self.config[\"input_width\"] / (2 ** 5)) ** 2 * layers[3])\n                        # fully connected linear layers\n                        n = 256 // fc_divider\n                        self.linear_layers = nn.Sequential(\n                            nn.Flatten(),\n                            nn.Linear(in_features=input_fc, out_features=n),\n                            nn.ReLU(),\n                            nn.Dropout2d(dropout),\n                            nn.Linear(in_features=n, out_features=1),\n                #             nn.ReLU(),\n                #             nn.Dropout2d(0.3),\n                #             nn.Linear(in_features=n, out_features=1),\n                        )\n\n                    def forward(self, x):\n                        x = self.conv_layers(x)\n                        # flatten to prepare for the fully connected layers\n                #         x = x.view(x.size(0), -1)\n                        x = self.linear_layers(x)\n                        return x\n\n                    def training_step(self, batch, batch_idx):\n                        x, y = batch\n                        y_hat = self(x)\n                #         weights = self.kde.score_samples(y_hat.cpu().detach().numpy())\n                        loss = self.criterion(y_hat, y)\n                        self.log(\n                            \"train_loss\",\n                            loss,\n                            on_step=False,\n                            on_epoch=True,\n                        )\n                        return loss\n\n                    def validation_step(self, batch, batch_idx):\n                        x, y = batch\n                        y_hat = self(x)\n                #         weights = self.kde.score_samples(y_hat.cpu().detach().numpy())\n                        metrics = {name: metric(y, y_hat) for name, metric in self.metrics.items()}\n                        metrics[\"val_loss\"] = self.criterion(y_hat, y)\n                        self.log_dict(metrics, on_step=False, on_epoch=True, prog_bar=True)\n                        return metrics\n\n                    def configure_criterion(self):\n                        self.criterion = self.L1Loss\n                        self.config[\"loss_type\"] = type(self.criterion)\n\n                    def L1Loss(self, y, y_hat, weights=None):\n                        if weights is not None:\n                            weights = nn.functional.normalize(weights, p=1, dim=0)\n                            return torch.mean(torch.matmul(weights, torch.abs(y - y_hat)))\n                        else: \n                            return torch.mean(torch.abs(y - y_hat))\n\n                    def configure_metrics(self):\n                        self.metrics = {\n                            \"mae\": torchmetrics.MeanAbsoluteError().to(self.config[\"device\"]),\n                            \"mape\": torchmetrics.MeanAbsolutePercentageError().to(\n                                self.config[\"device\"]\n                            ),\n                            \"smape\": torchmetrics.SymmetricMeanAbsolutePercentageError().to(\n                                self.config[\"device\"]\n                            ),\n                            \"r2_score\": torchmetrics.R2Score(num_outputs=23).to(self.config[\"device\"]),\n                            \"cosine_similarity\": torchmetrics.CosineSimilarity(reduction=\"mean\").to(\n                                self.config[\"device\"]\n                            ),\n                        }\n\n                    def configure_optimizers(self):\n                        optimizer = torch.optim.Adam(\n                            self.parameters(),\n                            lr=self.config[\"learning_rate\"],\n                            weight_decay=self.config[\"weight_decay\"],\n                        )\n                        self.config[\"optimizer_type\"] = type(optimizer)\n                        return optimizer\n\n                dm = DataModule(config, repo_path)\n\n                model = VGG11(config, kde=None)\n                total_params = sum(p.numel() for p in model.parameters())\n                print(f\"NOISE {noise}\\tFC DIVIDER {fc_divider}\\tLAYER DIVIDER {layer_divider}\\tDROPOUT {dropout}\")\n                print(f\"[INFO]: {total_params:,} total parameters.\")\n                model(torch.rand((2, 1, config[\"input_width\"], config[\"input_width\"])))\n\n                model_checkpoint = pl.callbacks.model_checkpoint.ModelCheckpoint(\n                    dirpath=run.dir,\n                    filename=\"{epoch}-{val_loss:.3f}\",\n                    monitor=\"val_loss\",\n                    mode=\"min\",\n                    verbose=True,\n                    save_last=True,\n                )\n\n    #             script_checkpoint = training.ScriptCheckpoint(\n    #                 dirpath=run.dir,\n    #             )\n\n    #             callbacks = [script_checkpoint]\n                callbacks = []\n                log = None\n                if run.job_type == \"train\" or True:\n                    callbacks.append(model_checkpoint)\n                    print(f\"[INFO]: saving models.\")\n                if run.job_type == \"debug\":\n                    log = \"all\"\n\n                if config[\"log_wandb\"]:\n                    wandb_logger = pl.loggers.WandbLogger()\n                    wandb_logger.watch(model, log=log, log_graph=True)\n                else:\n                    wandb_logger = None\n                trainer = pl.Trainer(\n                    max_epochs=30,\n                    callbacks=callbacks,\n                    logger=wandb_logger,\n                    devices=\"auto\", \n                    accelerator=\"auto\",\n                #     limit_train_batches=0.3, \n                #     limit_val_batches=0.3,\n                #     log_every_n_steps=1\n                )\n                trainer.fit(model, datamodule=dm, )\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}