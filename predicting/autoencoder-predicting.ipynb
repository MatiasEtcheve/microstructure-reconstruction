{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Autoencoder\n","\n","#### Objective\n","\n","The current task is to be able to reconstruct images from images. For this, we are going to use Autoencoders.\n","\n","#### Autoencoders\n","\n","An autoencoder is composed of a encoder and a decoder networks. The first network aims at reducing the complexity and the size of the input, whereas the decoder aims at reconstructing the input from the encoded data.\n","\n","#### Dataset\n","\n","The current inputs are images produced through the `custom_datasets.datasets.NWidthConcatPhotosDataset`. Indeed, this dataset is quite light but includes all the spatial complexity of the REVs.\n","\n","With such dataset, all the input elements are images of size `(1, 3*H, nb_image_per_axis*W)`: on the unique channel, there is 3 rows of white and black image corresponding to a sliced images taken along an axis.\n","Those rows are concatenated along the height.\n"," "]},{"cell_type":"markdown","metadata":{},"source":["# Importing the dataframe"]},{"cell_type":"markdown","metadata":{},"source":["Firstly, we initialize wandb. It is a tool that allows to store the losses and retrieve the deframe. Otherwise, you can directly access locally the dataframe on your computer."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T01:05:38.634462Z","iopub.status.busy":"2022-05-18T01:05:38.634183Z","iopub.status.idle":"2022-05-18T01:05:48.638129Z","shell.execute_reply":"2022-05-18T01:05:48.637225Z","shell.execute_reply.started":"2022-05-18T01:05:38.63443Z"},"executionInfo":{"elapsed":4758,"status":"ok","timestamp":1644495641039,"user":{"displayName":"Matias Etcheverry","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAaRgyKc_t8sZam6khtkO8IvEB4xqvz8dwKcJh=s64","userId":"13887759295001408411"},"user_tz":-60},"id":"KCF0gEhbI9yg","outputId":"65a442db-4731-422d-a7a4-ae8f237f375d","trusted":true},"outputs":[],"source":["!pip install wandb --upgrade"]},{"cell_type":"markdown","metadata":{},"source":["We import all the useful packages."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T01:05:48.641894Z","iopub.status.busy":"2022-05-18T01:05:48.641204Z","iopub.status.idle":"2022-05-18T01:05:48.654918Z","shell.execute_reply":"2022-05-18T01:05:48.653219Z","shell.execute_reply.started":"2022-05-18T01:05:48.641858Z"},"executionInfo":{"elapsed":12323,"status":"ok","timestamp":1644495653358,"user":{"displayName":"Matias Etcheverry","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAaRgyKc_t8sZam6khtkO8IvEB4xqvz8dwKcJh=s64","userId":"13887759295001408411"},"user_tz":-60},"id":"396f8318","outputId":"ecbf203b-7c9f-4285-a787-3c279f8be90f","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[INFO]: Computation device: cpu\n"]}],"source":["import sys\n","from pathlib import Path\n","\n","IS_COLAB = \"google.colab\" in sys.modules\n","IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n","if IS_KAGGLE:\n","    repo_path = Path(\"../input/microstructure-reconstruction\")\n","elif IS_COLAB:\n","    from google.colab import drive\n","\n","    drive.mount(\"/content/gdrive\")\n","    repo_path = Path(\"/content/gdrive/MyDrive/microstructure-reconstruction\")\n","else:\n","    repo_path = Path(\"/home/matias/microstructure-reconstruction\")\n","sys.path.append(str(repo_path))\n","\n","from copy import deepcopy\n","from importlib import reload\n","from typing import List, Union\n","\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import pytorch_lightning as pl\n","import torch\n","import torch.nn as nn\n","import wandb\n","from custom_datasets import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from tools import dataframe_reformat, inspect_code, plotting, training, wandb_api\n","from torch.utils.data import DataLoader\n","from torchvision import transforms, utils\n","\n","log_wandb = True\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","kwargs = {\"num_workers\": 2, \"pin_memory\": True} if use_cuda else {\"num_workers\": 4}\n","print(f\"[INFO]: Computation device: {device}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["We initialize a wandb run, that will save our metrics"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T01:05:48.657172Z","iopub.status.busy":"2022-05-18T01:05:48.656165Z","iopub.status.idle":"2022-05-18T01:05:53.943445Z","shell.execute_reply":"2022-05-18T01:05:53.942626Z","shell.execute_reply.started":"2022-05-18T01:05:48.657125Z"},"executionInfo":{"elapsed":6825,"status":"ok","timestamp":1644495660175,"user":{"displayName":"Matias Etcheverry","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAaRgyKc_t8sZam6khtkO8IvEB4xqvz8dwKcJh=s64","userId":"13887759295001408411"},"user_tz":-60},"id":"dj85A2mMIz_j","outputId":"91b1c0e2-dbca-4381-e5ac-9f404baf57db","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatiasetcheverry\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/matias/.netrc\n"]},{"data":{"text/html":["Tracking run with wandb version 0.12.16"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/matias/microstructure-reconstruction/predicting/wandb/run-20220524_221118-2g9phhif</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/matiasetcheverry/microstructure-reconstruction/runs/2g9phhif\" target=\"_blank\">efficient-totem-666</a></strong> to <a href=\"https://wandb.ai/matiasetcheverry/microstructure-reconstruction\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["if log_wandb:\n","    import wandb\n","\n","    wandb_api.login()\n","    run = wandb.init(\n","        project=\"microstructure-reconstruction\",\n","        group=\"Autoencoders\",\n","        job_type=\"test\",\n","    )\n"]},{"cell_type":"markdown","metadata":{},"source":["Parameters of our run:"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T01:05:53.947033Z","iopub.status.busy":"2022-05-18T01:05:53.946779Z","iopub.status.idle":"2022-05-18T01:05:53.969142Z","shell.execute_reply":"2022-05-18T01:05:53.968426Z","shell.execute_reply.started":"2022-05-18T01:05:53.947004Z"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1644495660176,"user":{"displayName":"Matias Etcheverry","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAaRgyKc_t8sZam6khtkO8IvEB4xqvz8dwKcJh=s64","userId":"13887759295001408411"},"user_tz":-60},"id":"w2Me9kwpIz_j","outputId":"f4290858-5c72-4331-ed8d-b99ffeb3b058","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Global seed set to 42\n"]},{"data":{"text/plain":["42"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["if log_wandb:\n","    config = wandb.config\n","else:\n","    config = {}\n","\n","config[\"job_type\"] = run.job_type if \"run\" in locals() else \"test\"\n","config[\"train_val_split\"] = 0.7\n","config[\"seed\"] = 42\n","config[\"batch_size\"] = 64\n","config[\"learning_rate\"] = 0.001\n","config[\"device\"] = device\n","config[\"architecture\"] = \"Autoencoder\"\n","config[\"sparse_term\"] = 0.1\n","config[\"weight_decay\"] = 0.0001\n","config[\"input_width\"] = 88\n","config[\"epochs\"] = 0\n","config[\"nb_image_per_axis\"] = 3\n","config[\"latent_size\"] = 1\n","config[\"log_wandb\"] = True\n","torch.manual_seed(config[\"seed\"])\n","pl.seed_everything(config[\"seed\"])\n"]},{"cell_type":"markdown","metadata":{},"source":["We retrieve the dataframe containing the descriptors."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T01:05:53.972418Z","iopub.status.busy":"2022-05-18T01:05:53.971451Z","iopub.status.idle":"2022-05-18T01:06:00.306954Z","shell.execute_reply":"2022-05-18T01:06:00.306119Z","shell.execute_reply.started":"2022-05-18T01:05:53.972371Z"},"executionInfo":{"elapsed":3755,"status":"ok","timestamp":1644495663926,"user":{"displayName":"Matias Etcheverry","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAaRgyKc_t8sZam6khtkO8IvEB4xqvz8dwKcJh=s64","userId":"13887759295001408411"},"user_tz":-60},"id":"9397b6e5","trusted":true},"outputs":[],"source":["class DataModule(pl.LightningDataModule):\n","    def __init__(\n","        self,\n","        config,\n","        repo_path,\n","        train_df=None,\n","        test_df=None,\n","        train_dataset=None,\n","        validation_dataset=None,\n","    ):\n","        super().__init__()\n","        self.config = config\n","        self.repo_path = repo_path\n","        self.train_df = train_df.convert_dtypes() if train_df is not None else None\n","        self.test_df = test_df.convert_dtypes() if test_df is not None else None\n","        self.train_dataset = train_dataset\n","        self.validation_dataset = validation_dataset\n","\n","        self.transform = transforms.Compose(\n","            [\n","                transforms.GaussianBlur(kernel_size=3, sigma=0.5),\n","            ]\n","        )\n","\n","    def init_df(self):\n","        train_df, test_df = wandb_api.fetch_train_test_df(\n","            alias=f\"{self.config['nb_image_per_axis']}_images_invariants\"\n","        )\n","        if self.train_df is None:\n","            self.train_df = train_df\n","        if self.test_df is None:\n","            self.test_df = test_df\n","\n","    def setup(self, stage):\n","        if self.train_dataset is None or self.validation_dataset is None:\n","            self.init_df()\n","\n","            self.scaler, [normalized_train_df, normalized_test_df] = wandb_api.normalize([self.train_df, self.test_df])\n","\n","            if self.train_dataset is None:\n","                self.train_dataset = datasets.NWidthConcatPhotosDataset(\n","                    normalized_train_df,\n","                    width=self.config[\"input_width\"],\n","                    nb_image_per_axis=config[\"nb_image_per_axis\"],\n","                    transform=self.transform,\n","                )\n","            if self.validation_dataset is None:\n","                self.validation_dataset = datasets.NWidthConcatPhotosDataset(\n","                    normalized_test_df,\n","                    width=self.config[\"input_width\"],\n","                    nb_image_per_axis=config[\"nb_image_per_axis\"],\n","                    transform=self.transform,\n","                )\n","            self.targets = self.test_df.iloc[:, 1:-1].to_numpy()\n","\n","    def train_dataloader(self):\n","        return DataLoader(\n","            self.train_dataset,\n","            batch_size=self.config[\"batch_size\"],\n","            shuffle=True,\n","            **kwargs,\n","        )\n","\n","    def val_dataloader(self):\n","        return DataLoader(\n","            self.validation_dataset,\n","            batch_size=self.config[\"batch_size\"],\n","            shuffle=False,\n","            **kwargs,\n","        )\n","\n","    def test_dataloader(self):\n","        return self.val_dataloader()\n","\n","    def predict_dataloader(self):\n","        return DataLoader(\n","            [image for image, _ in self.validation_dataset],\n","            batch_size=self.config[\"batch_size\"],\n","            shuffle=False,\n","            **kwargs,\n","        )\n","\n","\n","dm = DataModule(config, repo_path)\n","dm.prepare_data()\n","dm.setup(stage=\"fit\")\n","\n","first_batch = next(iter(dm.val_dataloader()))\n","images, labels = first_batch[0], first_batch[1]\n","\n","print(\"Length train dataset:\", len(dm.train_dataset))\n","print(\"Length val dataset:\", len(dm.validation_dataset))\n","print(\"Nb of descriptors:\", len(first_batch[1][0]))\n","print(\"Nb batch in dataset:\", len(dm.train_dataloader()))\n","print(\"Size of a batch:\", len(first_batch[0]))\n","print(\"Image shape:\", images[0].shape)\n","\n","grid = utils.make_grid(images)\n","fig = plt.figure(figsize=(90, 30))\n","plt.imshow(grid.numpy().transpose((1, 2, 0)))\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["# Networks definition\n","\n","The global autoencoder is made of 2 parts:\n","* the encoder, whose inputs are images and aims at encoding all the input images into vectors of length `config[\"latent_size\"]`. The encoder is compose of several blocks of:\n","    * `nn.Conv2d(kernel_size=5, stride=2, padding=0)`, \n","    * `nn.MaxPool2d()`, \n","    * `nn.LeakyReLU()`\n","* the decoder, whose inputs are the encoded vectors and aims at reconstructing all the input images of size `(config[\"input_width\"], config[\"input_width\"])`. The decoder is compose of several blocks of:\n","    * `nn.ConvTranspose2d(kernel_size=3, stride=1, padding=0,)`,\n","    * `nn.Upsample(scale_factor=(2, 2))`,\n","    * `nn.LeakyReLU()`,\n","    * `nn.BatchNorm2d()`,\n","\n","Note: \n","* we can notice that the decoder is made of the reverse opperations of the encoder.\n","\n","#### Encoder"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T01:06:00.309527Z","iopub.status.busy":"2022-05-18T01:06:00.308737Z","iopub.status.idle":"2022-05-18T01:06:00.898203Z","shell.execute_reply":"2022-05-18T01:06:00.897455Z","shell.execute_reply.started":"2022-05-18T01:06:00.309481Z"},"trusted":true},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mCanceled future for execute_request message before replies were done"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["class Encoder(nn.Module):\n","    def __init__(\n","        self,\n","        config,\n","    ):\n","        super(Encoder, self).__init__()\n","\n","        self.model = nn.Sequential(\n","            nn.Conv2d(1, 128, kernel_size=5, stride=2, padding=0),\n","            nn.MaxPool2d(2),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm2d(128),\n","            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=0),\n","            nn.MaxPool2d(2),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm2d(64),\n","            nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=0),\n","            nn.MaxPool2d(2),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm2d(32),\n","            nn.Flatten(),\n","        )\n","        _, length = self.model(\n","            torch.rand(\n","                (\n","                    config[\"batch_size\"],\n","                    1,\n","                    config[\"input_width\"],\n","                    config[\"input_width\"],\n","                )\n","            )\n","        ).shape\n","        self.output = nn.Sequential(\n","            nn.Linear(\n","                length,\n","                config[\"latent_size\"],\n","            ),\n","        )\n","\n","    def forward(self, img):\n","        x = self.model(img)\n","        return self.output(x)\n","\n","\n","descriptors = first_batch[1]\n","images = first_batch[0]\n","d = Encoder(config)\n","print(d(images).shape)\n","total_params = sum(p.numel() for p in d.parameters())\n","print(f\"[INFO]: {total_params:,} total parameters.\")\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Decoder"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T01:06:00.900796Z","iopub.status.busy":"2022-05-18T01:06:00.899967Z","iopub.status.idle":"2022-05-18T01:06:01.972047Z","shell.execute_reply":"2022-05-18T01:06:01.97133Z","shell.execute_reply.started":"2022-05-18T01:06:00.900754Z"},"trusted":true},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, config):\n","        super(Decoder, self).__init__()\n","        self.linear = nn.Linear(config[\"latent_size\"], 128)\n","        \n","        self.model = nn.Sequential(\n","            nn.ConvTranspose2d(\n","                32,\n","                32,\n","                kernel_size=3,\n","                stride=1,\n","                padding=0,\n","            ),\n","            nn.Upsample(scale_factor=(2, 2)),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm2d(32),\n","            \n","            nn.ConvTranspose2d(\n","                32,\n","                64,\n","                kernel_size=3,\n","                stride=1,\n","                padding=0,\n","            ),\n","            nn.Upsample(scale_factor=(2, 2)),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm2d(64),\n","            \n","            nn.ConvTranspose2d(\n","                64,\n","                128,\n","                kernel_size=5,\n","                stride=2,\n","                padding=0,\n","            ),\n","            nn.Upsample(scale_factor=(2, 2)),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm2d(128),\n","            \n","            nn.ConvTranspose2d(\n","                128,\n","                1,\n","                kernel_size=3,\n","                stride=1,\n","                padding=0,\n","            ),\n","            \n","            nn.Sigmoid(),\n","        )\n","\n","    def forward(self, z):\n","        z = self.linear(z)\n","        z = torch.reshape(z, (-1, 32, 2, 2))\n","        img = self.model(z)\n","        return img\n","\n","\n","descriptors = first_batch[1]\n","images = first_batch[0]\n","d = Decoder(config)\n","print(d(torch.rand((config[\"batch_size\"], config[\"latent_size\"]))).shape)\n","total_params = sum(p.numel() for p in d.parameters())\n","print(f\"[INFO]: {total_params:,} total parameters.\")\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Global Autoencoder"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T01:06:01.974126Z","iopub.status.busy":"2022-05-18T01:06:01.973635Z","iopub.status.idle":"2022-05-18T01:06:03.567381Z","shell.execute_reply":"2022-05-18T01:06:03.566687Z","shell.execute_reply.started":"2022-05-18T01:06:01.974088Z"},"trusted":true},"outputs":[],"source":["class Autoencoder(pl.LightningModule):\n","    def __init__(\n","        self,\n","        config,\n","    ):\n","        super().__init__()\n","        self.config = config\n","\n","        self.encoder = Encoder(config)\n","        self.decoder = Decoder(config)\n","\n","    def forward(self, x):\n","        z = self.encoder(x)\n","        x_hat = self.decoder(z)\n","        return x_hat\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(\n","            self.parameters(),\n","            lr=self.config[\"learning_rate\"],\n","            weight_decay=self.config[\"weight_decay\"],\n","        )\n","        return optimizer\n","\n","    def compute_metrics(self, img):\n","        encoding = self.encoder(img)\n","        fake_img = self.decoder(encoding)\n","        mae_encoding = torch.mean(torch.abs(encoding))\n","        metrics = {\n","            \"loss\": nn.MSELoss(reduction=\"mean\")(\n","                fake_img, img\n","            ), #+ self.config[\"sparse_term\"]*mae_encoding,\n","            \"mse\": nn.MSELoss(reduction=\"mean\")(fake_img, img),\n","            \"mae_encoding\": mae_encoding,\n","            \"bce\": nn.BCELoss(reduction=\"mean\")(fake_img, img),\n","        }\n","        return metrics\n","\n","    def training_step(self, batch, batch_idx):\n","        img, _ = batch\n","        metrics = {\n","            \"train_\" + metric_name: metric_value\n","            for metric_name, metric_value in self.compute_metrics(img).items()\n","        }\n","        self.log_dict(\n","            metrics,\n","            on_step=False,\n","            on_epoch=True,\n","            prog_bar=True,\n","        )\n","        return metrics[\"train_loss\"]\n","\n","    def validation_step(self, batch, batch_idx):\n","        img, _ = batch\n","        metrics = {\n","            \"val_\" + metric_name: metric_value\n","            for metric_name, metric_value in self.compute_metrics(img).items()\n","        }\n","        self.log_dict(metrics, on_step=False, on_epoch=True, prog_bar=True)\n","        return metrics\n","\n","    def training_epoch_end(self, outputs):\n","        self.config[\"epochs\"] += 1\n","\n","\n","model = Autoencoder(config)\n","print(model)\n","total_params = sum(p.numel() for p in model.parameters())\n","print(f\"[INFO]: {total_params:,} total parameters.\")\n","model.training_step(first_batch, 1)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Checkpoint\n","\n","We had 2 checkpoints to our training:\n","\n","* one for saving our model every time we have a minimum in the validation loss \n","* one for saving the model's and data module script"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T01:06:03.568917Z","iopub.status.busy":"2022-05-18T01:06:03.568605Z","iopub.status.idle":"2022-05-18T01:06:03.580083Z","shell.execute_reply":"2022-05-18T01:06:03.579246Z","shell.execute_reply.started":"2022-05-18T01:06:03.568875Z"},"trusted":true},"outputs":[],"source":["model_checkpoint = pl.callbacks.model_checkpoint.ModelCheckpoint(\n","    dirpath=run.dir if \"run\" in locals() else \"tmp/\",\n","    filename=\"{epoch}-{val_bce:.3f}\",\n","    monitor=\"val_bce\",\n","    mode=\"min\",\n","    verbose=True,\n","    save_last=True,\n",")\n","\n","\n","script_checkpoint = training.ScriptCheckpoint(\n","    dirpath=run.dir if \"run\" in locals() else \"tmp/\",\n",")\n","images_callback = training.AutoencoderGeneratedImagesCallback(\n","    images=first_batch[0].to(device), log_every_n_epochs=1\n",")\n","callbacks = [script_checkpoint, images_callback]\n","log = None\n","if config[\"job_type\"] == \"train\" or True:\n","    callbacks.append(model_checkpoint)\n","    print(f\"[INFO]: saving models.\")\n","else:\n","    print(f\"[INFO]: not saving models.\")\n","if config[\"job_type\"] == \"debug\":\n","    log = \"all\"\n"]},{"cell_type":"markdown","metadata":{},"source":["# Training\n","\n","We then train our model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T01:06:03.584263Z","iopub.status.busy":"2022-05-18T01:06:03.584042Z","iopub.status.idle":"2022-05-18T01:36:16.627477Z","shell.execute_reply":"2022-05-18T01:36:16.62518Z","shell.execute_reply.started":"2022-05-18T01:06:03.584237Z"},"trusted":true},"outputs":[],"source":["if config[\"log_wandb\"]:\n","    wandb_logger = pl.loggers.WandbLogger()\n","    wandb_logger.watch(model, log=log, log_graph=True)\n","else:\n","    wandb_logger = None\n","trainer = pl.Trainer(\n","    max_epochs=400,\n","    callbacks=callbacks,\n","    logger=wandb_logger,\n","    devices=\"auto\",\n","    accelerator=\"auto\",\n","    #     limit_train_batches=0.3,\n","    #     limit_val_batches=1,\n","    #     log_every_n_steps=1,\n",")\n","trainer.fit(\n","    model,\n","    datamodule=dm,\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T01:36:16.630529Z","iopub.status.busy":"2022-05-18T01:36:16.630164Z","iopub.status.idle":"2022-05-18T01:36:24.031878Z","shell.execute_reply":"2022-05-18T01:36:24.031198Z","shell.execute_reply.started":"2022-05-18T01:36:16.630484Z"},"trusted":true},"outputs":[],"source":["run.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T01:36:45.469938Z","iopub.status.busy":"2022-05-18T01:36:45.469662Z","iopub.status.idle":"2022-05-18T01:36:49.722778Z","shell.execute_reply":"2022-05-18T01:36:49.718482Z","shell.execute_reply.started":"2022-05-18T01:36:45.469909Z"},"trusted":true},"outputs":[],"source":["targets = next(iter(dm.train_dataloader()))[0] #first_batch[0]\n","# outputs = save_output.outputs[0].permute(1, 0, 2, 3).detach().cpu()[:30]\n","grid_img = utils.make_grid(targets, normalize=False, pad_value=1, padding=1)\n","plt.figure(figsize=(30, 30))\n","plt.imshow(grid_img.cpu().numpy().transpose(1,2,0))\n","plt.title(\"Targets images from first validation batch\", fontdict={'fontsize': 70})\n","plt.tight_layout()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T01:36:53.501926Z","iopub.status.busy":"2022-05-18T01:36:53.501407Z","iopub.status.idle":"2022-05-18T01:36:56.731148Z","shell.execute_reply":"2022-05-18T01:36:56.730441Z","shell.execute_reply.started":"2022-05-18T01:36:53.501886Z"},"trusted":true},"outputs":[],"source":["fake_img = model(targets.to(device))\n","outputs = fake_img #(-fake_img +1 > 0.6).float()*1\n","print(outputs.shape)\n","# outputs = save_output.outputs[0].permute(1, 0, 2, 3).detach().cpu()[:30]\n","grid_img = utils.make_grid(outputs, normalize=False, pad_value=1, padding=1)\n","plt.figure(figsize=(30, 30))\n","plt.imshow(grid_img.cpu().numpy().transpose(1,2,0))\n","plt.title(\"Reconstructed images from first validation batch\", fontdict={'fontsize': 70})\n","plt.tight_layout()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n"]}],"metadata":{"interpreter":{"hash":"d6c21a451cc458c1a3016905611a31410e72e63593dc177dd12bc1df669080b2"},"kernelspec":{"display_name":"Python 3.9.8 64-bit ('gt')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.8"}},"nbformat":4,"nbformat_minor":4}
