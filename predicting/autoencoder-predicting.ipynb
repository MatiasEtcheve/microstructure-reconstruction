{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Network per channel\n","\n","As we dive more precisely into the topic, we now create a more robust dataset:\n","\n","* each input is an image of size `(C, W, H)`. On each channel, there is a grayscale sliced image. So, \n","  * the first `C/3` channels are sliced images along x axis\n","  * the following `C/3` channels are sliced images along x axis\n","  * the last `C/3` channels are sliced images along x axis\n","* each output is list of fabric descriptors\n","\n","If we take `C=3`, we can use a pretrained VGG model. Indeed, this model is trained on RGB images, which have 3 channels."]},{"cell_type":"markdown","metadata":{},"source":["# Importing the dataframe"]},{"cell_type":"markdown","metadata":{},"source":["Firstly, we initialize wandb. It is a tool that allows to store the losses and retrieve the deframe. Otherwise, you can directly access locally the dataframe on your computer."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T01:05:38.634462Z","iopub.status.busy":"2022-05-18T01:05:38.634183Z","iopub.status.idle":"2022-05-18T01:05:48.638129Z","shell.execute_reply":"2022-05-18T01:05:48.637225Z","shell.execute_reply.started":"2022-05-18T01:05:38.63443Z"},"executionInfo":{"elapsed":4758,"status":"ok","timestamp":1644495641039,"user":{"displayName":"Matias Etcheverry","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAaRgyKc_t8sZam6khtkO8IvEB4xqvz8dwKcJh=s64","userId":"13887759295001408411"},"user_tz":-60},"id":"KCF0gEhbI9yg","outputId":"65a442db-4731-422d-a7a4-ae8f237f375d","trusted":true},"outputs":[],"source":["!pip install wandb --upgrade"]},{"cell_type":"markdown","metadata":{},"source":["We import all the useful packages."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T01:05:48.641894Z","iopub.status.busy":"2022-05-18T01:05:48.641204Z","iopub.status.idle":"2022-05-18T01:05:48.654918Z","shell.execute_reply":"2022-05-18T01:05:48.653219Z","shell.execute_reply.started":"2022-05-18T01:05:48.641858Z"},"executionInfo":{"elapsed":12323,"status":"ok","timestamp":1644495653358,"user":{"displayName":"Matias Etcheverry","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAaRgyKc_t8sZam6khtkO8IvEB4xqvz8dwKcJh=s64","userId":"13887759295001408411"},"user_tz":-60},"id":"396f8318","outputId":"ecbf203b-7c9f-4285-a787-3c279f8be90f","trusted":true},"outputs":[],"source":["import sys\n","from pathlib import Path\n","\n","IS_COLAB = \"google.colab\" in sys.modules\n","IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n","if IS_KAGGLE:\n","    repo_path = Path(\"../input/microstructure-reconstruction\")\n","elif IS_COLAB:\n","    from google.colab import drive\n","\n","    drive.mount(\"/content/gdrive\")\n","    repo_path = Path(\"/content/gdrive/MyDrive/microstructure-reconstruction\")\n","else:\n","    repo_path = Path(\"/home/matias/microstructure-reconstruction\")\n","sys.path.append(str(repo_path))\n","\n","from copy import deepcopy\n","from importlib import reload\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from PIL import Image\n","import pandas as pd\n","import pytorch_lightning as pl\n","import torch\n","from typing import Union, List\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchmetrics\n","import torchvision.models as pretrained_models\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KernelDensity\n","from sklearn.preprocessing import MinMaxScaler\n","from torch.utils.data import DataLoader\n","from torchvision import transforms, utils\n","from tqdm import tqdm\n","\n","import wandb\n","from custom_datasets import dataset\n","from custom_models import models\n","from tools import dataframe_reformat, inspect_code, plotting, training, wandb_api\n","\n","log_wandb = True\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","kwargs = {\"num_workers\": 2, \"pin_memory\": True} if use_cuda else {\"num_workers\": 4}\n","print(f\"[INFO]: Computation device: {device}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["We initialize a wandb run, that will save our metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T01:05:48.657172Z","iopub.status.busy":"2022-05-18T01:05:48.656165Z","iopub.status.idle":"2022-05-18T01:05:53.943445Z","shell.execute_reply":"2022-05-18T01:05:53.942626Z","shell.execute_reply.started":"2022-05-18T01:05:48.657125Z"},"executionInfo":{"elapsed":6825,"status":"ok","timestamp":1644495660175,"user":{"displayName":"Matias Etcheverry","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAaRgyKc_t8sZam6khtkO8IvEB4xqvz8dwKcJh=s64","userId":"13887759295001408411"},"user_tz":-60},"id":"dj85A2mMIz_j","outputId":"91b1c0e2-dbca-4381-e5ac-9f404baf57db","trusted":true},"outputs":[],"source":["if log_wandb:\n","    import wandb\n","\n","    wandb_api.login()\n","    run = wandb.init(\n","        project=\"microstructure-reconstruction\",\n","        group=\"Autoencoders\",\n","        job_type=\"test\",\n","    )\n"]},{"cell_type":"markdown","metadata":{},"source":["Parameters of our run:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T01:05:53.947033Z","iopub.status.busy":"2022-05-18T01:05:53.946779Z","iopub.status.idle":"2022-05-18T01:05:53.969142Z","shell.execute_reply":"2022-05-18T01:05:53.968426Z","shell.execute_reply.started":"2022-05-18T01:05:53.947004Z"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1644495660176,"user":{"displayName":"Matias Etcheverry","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAaRgyKc_t8sZam6khtkO8IvEB4xqvz8dwKcJh=s64","userId":"13887759295001408411"},"user_tz":-60},"id":"w2Me9kwpIz_j","outputId":"f4290858-5c72-4331-ed8d-b99ffeb3b058","trusted":true},"outputs":[],"source":["if log_wandb:\n","    config = wandb.config\n","else:\n","    config = {}\n","\n","config[\"job_type\"] = run.job_type if \"run\" in locals() else \"test\"\n","config[\"train_val_split\"] = 0.7\n","config[\"seed\"] = 42\n","config[\"batch_size\"] = 64\n","config[\"learning_rate\"] = 0.001\n","config[\"device\"] = device\n","config[\"architecture\"] = \"Autoencoder\"\n","config[\"sparse_term\"] = 0.1\n","config[\"weight_decay\"] = 0.0001\n","config[\"input_width\"] = 88\n","config[\"epochs\"] = 0\n","config[\"nb_image_per_axis\"] = 10\n","config[\"latent_size\"] = 1\n","config[\"log_wandb\"] = True\n","torch.manual_seed(config[\"seed\"])\n","pl.seed_everything(config[\"seed\"])\n"]},{"cell_type":"markdown","metadata":{},"source":["We retrieve the dataframe containing the descriptors. This can locally be done on your computer."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T01:05:53.972418Z","iopub.status.busy":"2022-05-18T01:05:53.971451Z","iopub.status.idle":"2022-05-18T01:06:00.306954Z","shell.execute_reply":"2022-05-18T01:06:00.306119Z","shell.execute_reply.started":"2022-05-18T01:05:53.972371Z"},"executionInfo":{"elapsed":3755,"status":"ok","timestamp":1644495663926,"user":{"displayName":"Matias Etcheverry","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAaRgyKc_t8sZam6khtkO8IvEB4xqvz8dwKcJh=s64","userId":"13887759295001408411"},"user_tz":-60},"id":"9397b6e5","trusted":true},"outputs":[],"source":["class DataModule(pl.LightningDataModule):\n","    def __init__(\n","        self,\n","        config,\n","        repo_path,\n","        train_df=None,\n","        test_df=None,\n","        train_dataset=None,\n","        validation_dataset=None,\n","    ):\n","        super().__init__()\n","        self.config = config\n","        self.repo_path = repo_path\n","        self.train_df = train_df.convert_dtypes() if train_df is not None else None\n","        self.test_df = test_df.convert_dtypes() if test_df is not None else None\n","        self.train_dataset = train_dataset\n","        self.validation_dataset = validation_dataset\n","\n","        if self.config[\"log_wandb\"]:\n","            if self.train_df is None:\n","                self.training_data_at = wandb.Api().artifact(\n","                    f\"matiasetcheverry/microstructure-reconstruction/train_df:{self.config['nb_image_per_axis']}_images_invariants\"\n","                )\n","            if self.test_df is None:\n","                self.test_data_at = wandb.Api().artifact(\n","                    f\"matiasetcheverry/microstructure-reconstruction/test_df:{self.config['nb_image_per_axis']}_images_invariants\"\n","                )\n","\n","        self.transform = transforms.Compose(\n","            [\n","                transforms.GaussianBlur(kernel_size=3, sigma=0.5),\n","            ]\n","        )\n","\n","    def prepare_data(self):\n","        if self.config[\"log_wandb\"]:\n","            if self.train_df is None:\n","                self.training_data_at.download()\n","            if self.test_df is None:\n","                self.test_data_at.download()\n","\n","    def _init_df_wandb(self):\n","        if self.train_df is None:\n","            self.train_df = wandb_api.convert_table_to_dataframe(\n","                self.training_data_at.get(\"fabrics\")\n","            )\n","            self.train_df[\"photos\"] = self.train_df[\"photos\"].apply(\n","                func=lambda photo_paths: [\n","                    str(self.repo_path / Path(x)) for x in photo_paths\n","                ]\n","            )\n","        if self.test_df is None:\n","            self.test_df = wandb_api.convert_table_to_dataframe(\n","                self.test_data_at.get(\"fabrics\")\n","            )\n","            self.test_df[\"photos\"] = self.test_df[\"photos\"].apply(\n","                func=lambda photo_paths: [\n","                    str(self.repo_path / Path(x)) for x in photo_paths\n","                ]\n","            )\n","\n","    def _init_df_local(self):\n","        fabrics_df = pd.read_csv(self.repo_path / \"REV1_600/fabrics.txt\")\n","        path_to_slices = self.repo_path / \"REV1_600/REV1_600Slices\"\n","        fabrics_df[\"photos\"] = fabrics_df[\"id\"].apply(\n","            func=dataframe_reformat.associate_rev_id_to_its_images,\n","            args=(path_to_slices, self.config[\"nb_image_per_axis\"]),\n","        )\n","        fabrics_df = fabrics_df[fabrics_df.photos.str.len().gt(0)]\n","        fabrics_df[\"photos\"] = fabrics_df[\"photos\"].apply(func=lambda x: sorted(x))\n","        train_df, test_df = train_test_split(\n","            fabrics_df,\n","            train_size=config[\"train_val_split\"],\n","            random_state=config[\"seed\"],\n","            shuffle=True,\n","        )\n","        if self.train_df is None:\n","            self.train_df = train_df.reset_index(drop=True)\n","        if self.test_df is None:\n","            self.test_df = test_df.reset_index(drop=True)\n","\n","    def init_df(self):\n","        if self.config[\"log_wandb\"]:\n","            self._init_df_wandb()\n","        else:\n","            self._init_df_local()\n","\n","    def setup(self, stage):\n","        if self.train_dataset is None or self.validation_dataset is None:\n","            self.init_df()\n","\n","            self.scaler = MinMaxScaler(feature_range=(0, 1))\n","            self.scaler.partial_fit(self.train_df.iloc[:, 1:-1])\n","            self.scaler.partial_fit(self.test_df.iloc[:, 1:-1])\n","\n","            normalized_train_df = deepcopy(self.train_df)\n","            normalized_train_df.iloc[:, 1:-1] = self.scaler.transform(\n","                self.train_df.iloc[:, 1:-1]\n","            )\n","            normalized_test_df = deepcopy(self.test_df)\n","            normalized_test_df.iloc[:, 1:-1] = self.scaler.transform(\n","                self.test_df.iloc[:, 1:-1]\n","            )\n","\n","            if self.train_dataset is None:\n","                self.train_dataset = dataset.SinglePhotoDataset(\n","                    normalized_train_df,\n","                    width=self.config[\"input_width\"],\n","                    # nb_image_per_axis=config[\"nb_image_per_axis\"],\n","                    transform=self.transform,\n","                    noise=0,\n","                )\n","            if self.validation_dataset is None:\n","                self.validation_dataset = dataset.SinglePhotoDataset(\n","                    normalized_test_df,\n","                    width=self.config[\"input_width\"],\n","                    # nb_image_per_axis=config[\"nb_image_per_axis\"],\n","                    transform=self.transform,\n","                    noise=0,\n","                )\n","            self.targets = self.test_df.iloc[:, 1:-1].to_numpy()\n","\n","    def train_dataloader(self):\n","        return DataLoader(\n","            self.train_dataset,\n","            batch_size=self.config[\"batch_size\"],\n","            shuffle=True,\n","            **kwargs,\n","        )\n","\n","    def val_dataloader(self):\n","        return DataLoader(\n","            self.validation_dataset,\n","            batch_size=self.config[\"batch_size\"],\n","            shuffle=False,\n","            **kwargs,\n","        )\n","\n","    def test_dataloader(self):\n","        return self.val_dataloader()\n","\n","    def predict_dataloader(self):\n","        return DataLoader(\n","            [image for image, _ in self.validation_dataset],\n","            batch_size=self.config[\"batch_size\"],\n","            shuffle=False,\n","            **kwargs,\n","        )\n","\n","\n","dm = DataModule(config, repo_path)\n","dm.prepare_data()\n","dm.setup(stage=\"fit\")\n","first_batch = next(iter(dm.val_dataloader()))\n","print(\"Nb of descriptors:\", len(first_batch[1][0]))\n","print(\"Nb batch in dataset:\", len(dm.train_dataloader()))\n","print(\"Size of a batch:\", len(first_batch[0]))\n","images, labels = first_batch[0], first_batch[1]\n","print(\"Image shape:\", images[0].shape)\n","grid = utils.make_grid(images)\n","fig = plt.figure(figsize=(90, 30))\n","plt.imshow(grid.numpy().transpose((1, 2, 0)))\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["# Network definition"]},{"cell_type":"markdown","metadata":{},"source":["The next step is to define our model. This model is inspired by VGG11:\n","\n","* we define several convulational blocks.\n","* each of this block is sequence of:\n","  * convulational layer with `kernel_size=3, padding=1`\n","  * activation function, here it is the `ReLU`\n","  * max pooling layer with `kernel_size=2, stride=2` which aims at reducing the size of the convolutional layers"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T01:06:00.309527Z","iopub.status.busy":"2022-05-18T01:06:00.308737Z","iopub.status.idle":"2022-05-18T01:06:00.898203Z","shell.execute_reply":"2022-05-18T01:06:00.897455Z","shell.execute_reply.started":"2022-05-18T01:06:00.309481Z"},"trusted":true},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(\n","        self,\n","        config,\n","    ):\n","        super(Encoder, self).__init__()\n","\n","        self.model = nn.Sequential(\n","            nn.Conv2d(1, 128, kernel_size=5, stride=2, padding=0),\n","            nn.MaxPool2d(2),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm2d(128),\n","            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=0),\n","            nn.MaxPool2d(2),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm2d(64),\n","            nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=0),\n","            nn.MaxPool2d(2),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm2d(32),\n","            nn.Flatten(),\n","#             nn.Dropout(p=0.3),\n","        )\n","        _, length = self.model(\n","            torch.rand(\n","                (\n","                    config[\"batch_size\"],\n","                    1,\n","                    config[\"input_width\"],\n","                    config[\"input_width\"],\n","                )\n","            )\n","        ).shape\n","        self.output = nn.Sequential(\n","            nn.Linear(\n","                length,\n","                config[\"latent_size\"],\n","            ),\n","#             nn.Tanh(),\n","        )\n","\n","    def forward(self, img):\n","        x = self.model(img)\n","        return self.output(x)\n","\n","\n","descriptors = first_batch[1]\n","images = first_batch[0]\n","d = Encoder(config)\n","print(d(images).shape)\n","total_params = sum(p.numel() for p in d.parameters())\n","print(f\"[INFO]: {total_params:,} total parameters.\")\n","# g = Generator(config)\n","# print(g(descriptors).shape)\n","# total_params = sum(p.numel() for p in g.parameters())\n","# print(f\"[INFO]: {total_params:,} total parameters.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T01:06:00.900796Z","iopub.status.busy":"2022-05-18T01:06:00.899967Z","iopub.status.idle":"2022-05-18T01:06:01.972047Z","shell.execute_reply":"2022-05-18T01:06:01.97133Z","shell.execute_reply.started":"2022-05-18T01:06:00.900754Z"},"trusted":true},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, config, latent_dim=28):\n","        super(Decoder, self).__init__()\n","        self.linear = nn.Linear(config[\"latent_size\"], 128)\n","        \n","        self.model = nn.Sequential(\n","            nn.ConvTranspose2d(\n","                32,\n","                32,\n","                kernel_size=3,\n","                stride=1,\n","                padding=0,\n","            ),\n","            nn.Upsample(scale_factor=(2, 2)),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm2d(32),\n","            \n","            nn.ConvTranspose2d(\n","                32,\n","                64,\n","                kernel_size=3,\n","                stride=1,\n","                padding=0,\n","            ),\n","            nn.Upsample(scale_factor=(2, 2)),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm2d(64),\n","            \n","            nn.ConvTranspose2d(\n","                64,\n","                128,\n","                kernel_size=5,\n","                stride=2,\n","                padding=0,\n","            ),\n","            nn.Upsample(scale_factor=(2, 2)),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm2d(128),\n","            \n","            nn.ConvTranspose2d(\n","                128,\n","                1,\n","                kernel_size=3,\n","                stride=1,\n","                padding=0,\n","            ),\n","            \n","            nn.Sigmoid(),\n","        )\n","\n","    def forward(self, z):\n","        z = self.linear(z)\n","        z = torch.reshape(z, (-1, 32, 2, 2))\n","        img = self.model(z)\n","        return img\n","\n","\n","descriptors = first_batch[1]\n","images = first_batch[0]\n","d = Decoder(config)\n","print(d(torch.rand((config[\"batch_size\"], config[\"latent_size\"]))).shape)\n","total_params = sum(p.numel() for p in d.parameters())\n","print(f\"[INFO]: {total_params:,} total parameters.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T01:06:01.974126Z","iopub.status.busy":"2022-05-18T01:06:01.973635Z","iopub.status.idle":"2022-05-18T01:06:03.567381Z","shell.execute_reply":"2022-05-18T01:06:03.566687Z","shell.execute_reply.started":"2022-05-18T01:06:01.974088Z"},"trusted":true},"outputs":[],"source":["class Autoencoder(pl.LightningModule):\n","    def __init__(\n","        self,\n","        config,\n","    ):\n","        super().__init__()\n","        self.config = config\n","\n","        self.encoder = Encoder(config)\n","        self.decoder = Decoder(config)\n","\n","    #         self.configure_criterion()\n","    #         self.configure_metrics()\n","\n","    def forward(self, x):\n","        z = self.encoder(x)\n","        x_hat = self.decoder(z)\n","        return x_hat\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(\n","            self.parameters(),\n","            lr=self.config[\"learning_rate\"],\n","            weight_decay=self.config[\"weight_decay\"],\n","        )\n","        return optimizer\n","\n","    def compute_metrics(self, img):\n","        encoding = self.encoder(img)\n","        fake_img = self.decoder(encoding)\n","        mae_encoding = torch.mean(torch.abs(encoding))\n","        metrics = {\n","            \"loss\": nn.MSELoss(reduction=\"mean\")(\n","                fake_img, img\n","            ), #+ self.config[\"sparse_term\"]*mae_encoding,\n","            \"mse\": nn.MSELoss(reduction=\"mean\")(fake_img, img),\n","            \"mae_encoding\": mae_encoding,\n","            \"bce\": nn.BCELoss(reduction=\"mean\")(fake_img, img),\n","        }\n","        return metrics\n","\n","    def training_step(self, batch, batch_idx):\n","        img, _ = batch\n","        metrics = {\n","            \"train_\" + metric_name: metric_value\n","            for metric_name, metric_value in self.compute_metrics(img).items()\n","        }\n","        self.log_dict(\n","            metrics,\n","            on_step=False,\n","            on_epoch=True,\n","            prog_bar=True,\n","        )\n","        return metrics[\"train_loss\"]\n","\n","    def validation_step(self, batch, batch_idx):\n","        img, _ = batch\n","        metrics = {\n","            \"val_\" + metric_name: metric_value\n","            for metric_name, metric_value in self.compute_metrics(img).items()\n","        }\n","        self.log_dict(metrics, on_step=False, on_epoch=True, prog_bar=True)\n","        return metrics\n","\n","    def training_epoch_end(self, outputs):\n","        self.config[\"epochs\"] += 1\n","\n","\n","model = Autoencoder(config)\n","print(model)\n","total_params = sum(p.numel() for p in model.parameters())\n","print(f\"[INFO]: {total_params:,} total parameters.\")\n","model.training_step(first_batch, 1)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Checkpoint\n","\n","We had 2 checkpoints to our training:\n","\n","* one for saving our model every time we have a minimum in the validation loss \n","* one for saving the model's and data module script"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T01:06:03.568917Z","iopub.status.busy":"2022-05-18T01:06:03.568605Z","iopub.status.idle":"2022-05-18T01:06:03.580083Z","shell.execute_reply":"2022-05-18T01:06:03.579246Z","shell.execute_reply.started":"2022-05-18T01:06:03.568875Z"},"trusted":true},"outputs":[],"source":["model_checkpoint = pl.callbacks.model_checkpoint.ModelCheckpoint(\n","    dirpath=run.dir if \"run\" in locals() else \"tmp/\",\n","    filename=\"{epoch}-{val_bce:.3f}\",\n","    monitor=\"val_bce\",\n","    mode=\"min\",\n","    verbose=True,\n","    save_last=True,\n",")\n","\n","\n","script_checkpoint = training.ScriptCheckpoint(\n","    dirpath=run.dir if \"run\" in locals() else \"tmp/\",\n",")\n","images_callback = training.AutoencoderGeneratedImagesCallback(\n","    images=first_batch[0].to(device), log_every_n_epochs=1\n",")\n","callbacks = [script_checkpoint, images_callback]\n","log = None\n","if config[\"job_type\"] == \"train\" or True:\n","    callbacks.append(model_checkpoint)\n","    print(f\"[INFO]: saving models.\")\n","else:\n","    print(f\"[INFO]: not saving models.\")\n","if config[\"job_type\"] == \"debug\":\n","    log = \"all\"\n"]},{"cell_type":"markdown","metadata":{},"source":["# Training\n","\n","We then train our model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T01:06:03.584263Z","iopub.status.busy":"2022-05-18T01:06:03.584042Z","iopub.status.idle":"2022-05-18T01:36:16.627477Z","shell.execute_reply":"2022-05-18T01:36:16.62518Z","shell.execute_reply.started":"2022-05-18T01:06:03.584237Z"},"trusted":true},"outputs":[],"source":["if config[\"log_wandb\"]:\n","    wandb_logger = pl.loggers.WandbLogger()\n","    wandb_logger.watch(model, log=log, log_graph=True)\n","else:\n","    wandb_logger = None\n","trainer = pl.Trainer(\n","    max_epochs=400,\n","    callbacks=callbacks,\n","    logger=wandb_logger,\n","    devices=\"auto\",\n","    accelerator=\"auto\",\n","    #     limit_train_batches=0.3,\n","    #     limit_val_batches=1,\n","    #     log_every_n_steps=1,\n",")\n","trainer.fit(\n","    model,\n","    datamodule=dm,\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T01:36:16.630529Z","iopub.status.busy":"2022-05-18T01:36:16.630164Z","iopub.status.idle":"2022-05-18T01:36:24.031878Z","shell.execute_reply":"2022-05-18T01:36:24.031198Z","shell.execute_reply.started":"2022-05-18T01:36:16.630484Z"},"trusted":true},"outputs":[],"source":["run.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T01:36:45.469938Z","iopub.status.busy":"2022-05-18T01:36:45.469662Z","iopub.status.idle":"2022-05-18T01:36:49.722778Z","shell.execute_reply":"2022-05-18T01:36:49.718482Z","shell.execute_reply.started":"2022-05-18T01:36:45.469909Z"},"trusted":true},"outputs":[],"source":["targets = next(iter(dm.train_dataloader()))[0] #first_batch[0]\n","# outputs = save_output.outputs[0].permute(1, 0, 2, 3).detach().cpu()[:30]\n","grid_img = utils.make_grid(targets, normalize=False, pad_value=1, padding=1)\n","plt.figure(figsize=(30, 30))\n","plt.imshow(grid_img.cpu().numpy().transpose(1,2,0))\n","plt.title(\"Targets images from first validation batch\", fontdict={'fontsize': 70})\n","plt.tight_layout()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T01:36:53.501926Z","iopub.status.busy":"2022-05-18T01:36:53.501407Z","iopub.status.idle":"2022-05-18T01:36:56.731148Z","shell.execute_reply":"2022-05-18T01:36:56.730441Z","shell.execute_reply.started":"2022-05-18T01:36:53.501886Z"},"trusted":true},"outputs":[],"source":["fake_img = model(targets.to(device))\n","outputs = fake_img #(-fake_img +1 > 0.6).float()*1\n","print(outputs.shape)\n","# outputs = save_output.outputs[0].permute(1, 0, 2, 3).detach().cpu()[:30]\n","grid_img = utils.make_grid(outputs, normalize=False, pad_value=1, padding=1)\n","plt.figure(figsize=(30, 30))\n","plt.imshow(grid_img.cpu().numpy().transpose(1,2,0))\n","plt.title(\"Reconstructed images from first validation batch\", fontdict={'fontsize': 70})\n","plt.tight_layout()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
