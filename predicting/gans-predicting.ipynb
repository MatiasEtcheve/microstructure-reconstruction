{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Network per channel\n\nAs we dive more precisely into the topic, we now create a more robust dataset:\n\n* each input is an image of size `(C, W, H)`. On each channel, there is a grayscale sliced image. So, \n  * the first `C/3` channels are sliced images along x axis\n  * the following `C/3` channels are sliced images along x axis\n  * the last `C/3` channels are sliced images along x axis\n* each output is list of fabric descriptors\n\nIf we take `C=3`, we can use a pretrained VGG model. Indeed, this model is trained on RGB images, which have 3 channels.","metadata":{}},{"cell_type":"markdown","source":"# Importing the dataframe","metadata":{}},{"cell_type":"markdown","source":"Firstly, we initialize wandb. It is a tool that allows to store the losses and retrieve the deframe. Otherwise, you can directly access locally the dataframe on your computer.","metadata":{}},{"cell_type":"code","source":"!pip install wandb --upgrade","metadata":{"executionInfo":{"elapsed":4758,"status":"ok","timestamp":1644495641039,"user":{"displayName":"Matias Etcheverry","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAaRgyKc_t8sZam6khtkO8IvEB4xqvz8dwKcJh=s64","userId":"13887759295001408411"},"user_tz":-60},"id":"KCF0gEhbI9yg","outputId":"65a442db-4731-422d-a7a4-ae8f237f375d","execution":{"iopub.status.busy":"2022-05-05T20:24:05.82942Z","iopub.execute_input":"2022-05-05T20:24:05.830219Z","iopub.status.idle":"2022-05-05T20:24:17.88511Z","shell.execute_reply.started":"2022-05-05T20:24:05.830172Z","shell.execute_reply":"2022-05-05T20:24:17.884152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We import all the useful packages.","metadata":{}},{"cell_type":"code","source":"import sys\nfrom pathlib import Path\n\nIS_COLAB = \"google.colab\" in sys.modules\nIS_KAGGLE = \"kaggle_secrets\" in sys.modules\nif IS_KAGGLE:\n    repo_path = Path(\"../input/microstructure-reconstruction\")\nelif IS_COLAB:\n    from google.colab import drive\n\n    drive.mount(\"/content/gdrive\")\n    repo_path = Path(\"/content/gdrive/MyDrive/microstructure-reconstruction\")\nelse:\n    repo_path = Path(\"/home/matias/microstructure-reconstruction\")\nsys.path.append(str(repo_path))\n\nfrom copy import deepcopy\nfrom importlib import reload\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\nimport pandas as pd\nimport pytorch_lightning as pl\nimport torch\nfrom typing import Union, List\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchmetrics\nimport torchvision.models as pretrained_models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KernelDensity\nfrom sklearn.preprocessing import MinMaxScaler\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms, utils\nfrom tqdm import tqdm\n\nimport wandb\nfrom custom_datasets import dataset\nfrom custom_models import models\nfrom tools import dataframe_reformat, inspect_code, plotting, training, wandb_api\n\nlog_wandb = True\n\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\nkwargs = {\"num_workers\": 2, \"pin_memory\": True} if use_cuda else {\"num_workers\": 4}\nprint(f\"[INFO]: Computation device: {device}\")\n","metadata":{"executionInfo":{"elapsed":12323,"status":"ok","timestamp":1644495653358,"user":{"displayName":"Matias Etcheverry","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAaRgyKc_t8sZam6khtkO8IvEB4xqvz8dwKcJh=s64","userId":"13887759295001408411"},"user_tz":-60},"id":"396f8318","outputId":"ecbf203b-7c9f-4285-a787-3c279f8be90f","execution":{"iopub.status.busy":"2022-05-05T20:24:17.88856Z","iopub.execute_input":"2022-05-05T20:24:17.888964Z","iopub.status.idle":"2022-05-05T20:24:22.693387Z","shell.execute_reply.started":"2022-05-05T20:24:17.88893Z","shell.execute_reply":"2022-05-05T20:24:22.692487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We initialize a wandb run, that will save our metrics","metadata":{}},{"cell_type":"code","source":"if log_wandb:\n    import wandb\n\n    wandb_api.login()\n    run = wandb.init(\n        project=\"microstructure-reconstruction\",\n        group=\"Gans\",\n        job_type=\"test\",\n    )\n","metadata":{"executionInfo":{"elapsed":6825,"status":"ok","timestamp":1644495660175,"user":{"displayName":"Matias Etcheverry","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAaRgyKc_t8sZam6khtkO8IvEB4xqvz8dwKcJh=s64","userId":"13887759295001408411"},"user_tz":-60},"id":"dj85A2mMIz_j","outputId":"91b1c0e2-dbca-4381-e5ac-9f404baf57db","execution":{"iopub.status.busy":"2022-05-05T20:24:22.694964Z","iopub.execute_input":"2022-05-05T20:24:22.695247Z","iopub.status.idle":"2022-05-05T20:24:30.049083Z","shell.execute_reply.started":"2022-05-05T20:24:22.695212Z","shell.execute_reply":"2022-05-05T20:24:30.04839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Parameters of our run:","metadata":{}},{"cell_type":"code","source":"if log_wandb:\n    config = wandb.config\nelse:\n    config = wandb_api.Config()\n\nconfig[\"job_type\"] = run.job_type\nconfig[\"train_val_split\"] = 0.7\nconfig[\"seed\"] = 42\nconfig[\"batch_size\"] = 32\nconfig[\"learning_rate_generator\"] = 0.00001\nconfig[\"learning_rate_discriminator\"] = 0.0001\nconfig[\"device\"] = device\nconfig[\"architecture\"] = \"GANS\"\nconfig[\"input_width\"] = 64\nconfig[\"epochs\"] = 0\nconfig[\"nb_image_per_axis\"] = 1\nconfig[\"log_wandb\"] = True\nconfig[\"beta1\"] = 0.5\nconfig[\"beta2\"] = 0.9\nconfig[\"n_critic\"] = 1\ntorch.manual_seed(config[\"seed\"])\npl.seed_everything(config[\"seed\"])\n","metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1644495660176,"user":{"displayName":"Matias Etcheverry","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAaRgyKc_t8sZam6khtkO8IvEB4xqvz8dwKcJh=s64","userId":"13887759295001408411"},"user_tz":-60},"id":"w2Me9kwpIz_j","outputId":"f4290858-5c72-4331-ed8d-b99ffeb3b058","execution":{"iopub.status.busy":"2022-05-05T20:24:30.051768Z","iopub.execute_input":"2022-05-05T20:24:30.052165Z","iopub.status.idle":"2022-05-05T20:24:30.072961Z","shell.execute_reply.started":"2022-05-05T20:24:30.052135Z","shell.execute_reply":"2022-05-05T20:24:30.072084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We retrieve the dataframe containing the descriptors. This can locally be done on your computer.","metadata":{}},{"cell_type":"code","source":"class DataModule(pl.LightningDataModule):\n    def __init__(\n        self,\n        config,\n        repo_path,\n        train_df=None,\n        test_df=None,\n        train_dataset=None,\n        validation_dataset=None,\n    ):\n        super().__init__()\n        self.config = config\n        self.repo_path = repo_path\n        self.train_df = train_df.convert_dtypes() if train_df is not None else None\n        self.test_df = test_df.convert_dtypes() if test_df is not None else None\n        self.train_dataset = train_dataset\n        self.validation_dataset = validation_dataset\n\n        if self.config[\"log_wandb\"]:\n            if self.train_df is None:\n                self.training_data_at = wandb.Api().artifact(\n                    f\"matiasetcheverry/microstructure-reconstruction/train_df:{self.config['nb_image_per_axis']}_images_invariants\"\n                )\n            if self.test_df is None:\n                self.test_data_at = wandb.Api().artifact(\n                    f\"matiasetcheverry/microstructure-reconstruction/test_df:{self.config['nb_image_per_axis']}_images_invariants\"\n                )\n\n        self.transform = transforms.Compose(\n            [\n                transforms.GaussianBlur(kernel_size=3, sigma=0.5),\n            ]\n        )\n\n    def prepare_data(self):\n        if self.config[\"log_wandb\"]:\n            if self.train_df is None:\n                self.training_data_at.download()\n            if self.test_df is None:\n                self.test_data_at.download()\n\n    def _init_df_wandb(self):\n        if self.train_df is None:\n            self.train_df = wandb_api.convert_table_to_dataframe(\n                self.training_data_at.get(\"fabrics\")\n            )\n            self.train_df[\"photos\"] = self.train_df[\"photos\"].apply(\n                func=lambda photo_paths: [\n                    str(self.repo_path / Path(x)) for x in photo_paths\n                ]\n            )\n        if self.test_df is None:\n            self.test_df = wandb_api.convert_table_to_dataframe(\n                self.test_data_at.get(\"fabrics\")\n            )\n            self.test_df[\"photos\"] = self.test_df[\"photos\"].apply(\n                func=lambda photo_paths: [\n                    str(self.repo_path / Path(x)) for x in photo_paths\n                ]\n            )\n\n    def _init_df_local(self):\n        fabrics_df = pd.read_csv(self.repo_path / \"REV1_600/fabrics.txt\")\n        path_to_slices = self.repo_path / \"REV1_600/REV1_600Slices\"\n        fabrics_df[\"photos\"] = fabrics_df[\"id\"].apply(\n            func=dataframe_reformat.associate_rev_id_to_its_images,\n            args=(path_to_slices, self.config[\"nb_image_per_axis\"]),\n        )\n        fabrics_df = fabrics_df[fabrics_df.photos.str.len().gt(0)]\n        fabrics_df[\"photos\"] = fabrics_df[\"photos\"].apply(func=lambda x: sorted(x))\n        train_df, test_df = train_test_split(\n            fabrics_df,\n            train_size=config[\"train_val_split\"],\n            random_state=config[\"seed\"],\n            shuffle=True,\n        )\n        if self.train_df is None:\n            self.train_df = train_df.reset_index(drop=True)\n        if self.test_df is None:\n            self.test_df = test_df.reset_index(drop=True)\n\n    def init_df(self):\n        if self.config[\"log_wandb\"]:\n            self._init_df_wandb()\n        else:\n            self._init_df_local()\n\n    def setup(self, stage):\n        if self.train_dataset is None or self.validation_dataset is None:\n            self.init_df()\n\n            self.scaler = MinMaxScaler(feature_range=(0, 1))\n            self.scaler.partial_fit(self.train_df.iloc[:, 1:-1])\n            self.scaler.partial_fit(self.test_df.iloc[:, 1:-1])\n\n            normalized_train_df = deepcopy(self.train_df)\n            normalized_train_df.iloc[:, 1:-1] = self.scaler.transform(\n                self.train_df.iloc[:, 1:-1]\n            )\n            normalized_test_df = deepcopy(self.test_df)\n            normalized_test_df.iloc[:, 1:-1] = self.scaler.transform(\n                self.test_df.iloc[:, 1:-1]\n            )\n\n            if self.train_dataset is None:\n                self.train_dataset = dataset.NWidthStackedPhotosDataset(\n                    normalized_train_df,\n                    width=self.config[\"input_width\"],\n                    nb_image_per_axis=config[\"nb_image_per_axis\"],\n                    transform=self.transform,\n                    noise=0,\n                )\n            if self.validation_dataset is None:\n                self.validation_dataset = dataset.NWidthStackedPhotosDataset(\n                    normalized_test_df,\n                    width=self.config[\"input_width\"],\n                    nb_image_per_axis=config[\"nb_image_per_axis\"],\n                    transform=self.transform,\n                    noise=0,\n                )\n            self.targets = self.test_df.iloc[:, 1:-1].to_numpy()\n\n    def train_dataloader(self):\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.config[\"batch_size\"],\n            shuffle=True,\n            **kwargs,\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.validation_dataset,\n            batch_size=self.config[\"batch_size\"],\n            shuffle=False,\n            **kwargs,\n        )\n\n    def test_dataloader(self):\n        return self.val_dataloader()\n\n    def predict_dataloader(self):\n        return DataLoader(\n            [image for image, _ in self.validation_dataset],\n            batch_size=self.config[\"batch_size\"],\n            shuffle=False,\n            **kwargs,\n        )\n\n\ndm = DataModule(config, repo_path)\ndm.prepare_data()\ndm.setup(stage=\"fit\")\nfirst_batch = next(iter(dm.train_dataloader()))\nprint(\"Nb of descriptors:\", len(first_batch[1][0]))\nprint(\"Nb batch in dataset:\", len(dm.train_dataloader()))\nprint(\"Size of a batch:\", len(first_batch[0]))\nimages, labels = first_batch[0], first_batch[1]\nprint(\"Image shape:\", images[0].shape)\ngrid = utils.make_grid(images)\nfig = plt.figure(figsize=(90, 30))\nplt.imshow(grid.numpy().transpose((1, 2, 0)))\nplt.show()\n","metadata":{"executionInfo":{"elapsed":3755,"status":"ok","timestamp":1644495663926,"user":{"displayName":"Matias Etcheverry","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAaRgyKc_t8sZam6khtkO8IvEB4xqvz8dwKcJh=s64","userId":"13887759295001408411"},"user_tz":-60},"id":"9397b6e5","execution":{"iopub.status.busy":"2022-05-05T20:24:30.0745Z","iopub.execute_input":"2022-05-05T20:24:30.074887Z","iopub.status.idle":"2022-05-05T20:24:38.428292Z","shell.execute_reply.started":"2022-05-05T20:24:30.074852Z","shell.execute_reply":"2022-05-05T20:24:38.427458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = wandb.Image(\n    first_batch[0],\n    caption=f\"First batch target\",\n)\nwandb.log({\"generated_images\": image})","metadata":{"execution":{"iopub.status.busy":"2022-05-05T20:24:38.429636Z","iopub.execute_input":"2022-05-05T20:24:38.43044Z","iopub.status.idle":"2022-05-05T20:24:38.634638Z","shell.execute_reply.started":"2022-05-05T20:24:38.430405Z","shell.execute_reply":"2022-05-05T20:24:38.633811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Network definition","metadata":{}},{"cell_type":"markdown","source":"The next step is to define our model. This model is inspired by VGG11:\n\n* we define several convulational blocks.\n* each of this block is sequence of:\n  * convulational layer with `kernel_size=3, padding=1`\n  * activation function, here it is the `ReLU`\n  * max pooling layer with `kernel_size=2, stride=2` which aims at reducing the size of the convolutional layers","metadata":{}},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, config, latent_dim=28):\n        super(Generator, self).__init__()\n\n        kernel_sizes = [3, 3, 3, 3, 4]\n        strides = [1, 2, 2, 2, 2]\n        paddings = [0, 0, 0, 0, 0]\n        filters = [400, 512, 256, 64, 32, 3]\n\n        self.linear_layer = nn.Sequential(\n            nn.Linear(latent_dim, filters[0]),\n            nn.ReLU(),\n        )\n        self.model = nn.Sequential(\n            nn.ConvTranspose2d(\n                filters[0],\n                filters[0],\n                stride=2,\n                kernel_size=3,\n                padding=paddings[0],\n            ),\n            nn.Conv2d(\n                filters[0],\n                filters[1],\n                kernel_size=3, stride=1, padding=1,\n            ),\n            nn.BatchNorm2d(filters[1]),\n            nn.ReLU(),\n            \n            nn.ConvTranspose2d(\n                filters[1],\n                filters[1],\n                stride=2,\n                kernel_size=3,\n                padding=paddings[0],\n            ),\n            nn.Conv2d(\n                filters[1],\n                filters[2],\n                kernel_size=3, stride=1, padding=1,\n            ),\n            nn.BatchNorm2d(filters[2]),\n            nn.ReLU(),\n            \n            nn.ConvTranspose2d(\n                filters[2],\n                filters[2],\n                stride=2,\n                kernel_size=3,\n                padding=paddings[0],\n            ),\n            nn.Conv2d(\n                filters[2],\n                filters[3],\n                kernel_size=3, stride=1, padding=1,\n            ),\n            nn.BatchNorm2d(filters[3]),\n            nn.ReLU(),\n            \n            nn.ConvTranspose2d(\n                filters[3],\n                filters[3],\n                stride=2,\n                kernel_size=4,\n                padding=paddings[0],\n            ),\n            nn.Conv2d(\n                filters[3],\n                filters[4],\n                kernel_size=3, stride=1, padding=1,\n            ),\n            nn.BatchNorm2d(filters[4]),\n            \n            nn.ConvTranspose2d(\n                filters[4],\n                filters[4],\n                stride=2,\n                kernel_size=4,\n                padding=1,\n            ),\n            nn.Conv2d(\n                filters[4],\n                filters[5],\n                kernel_size=3, stride=1, padding=1,\n            ),\n            nn.BatchNorm2d(filters[5]),\n            nn.ReLU(),\n            nn.Tanh(),\n        )\n\n    def forward(self, z):\n        z = self.linear_layer(z)\n        z = torch.unsqueeze(torch.unsqueeze(z, -1), -1)\n        img = self.model(z)\n        return img\n\nclass Discriminator(nn.Module):\n    def __init__(\n        self,\n        config,\n    ):\n        super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Conv2d(\n                3 * config[\"nb_image_per_axis\"], 512, kernel_size=3, stride=1, padding=1\n            ),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(),\n            nn.Conv2d(512, 256, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(),\n            nn.Conv2d(256, 126, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(126),\n            nn.LeakyReLU(),\n            nn.Conv2d(126, 64, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(),\n            nn.Flatten(),\n        )\n        _, length = self.model(\n            torch.rand(\n                (\n                    config[\"batch_size\"],\n                    3 * config[\"nb_image_per_axis\"],\n                    config[\"input_width\"],\n                    config[\"input_width\"],\n                )\n            )\n        ).shape\n        self.output = nn.Sequential(\n            nn.Linear(\n                length,\n                1,\n            ),\n            nn.Sigmoid()\n        )\n\n    def forward(self, img):\n        x = self.model(img)\n        return self.output(x)\n\n\nclass WGANGP(pl.LightningModule):\n    def __init__(\n        self,\n        config,\n    ):\n        super().__init__()\n        self.config = config\n\n        self.generator = Generator(config, latent_dim=28)\n        self.discriminator = Discriminator(config)\n\n    def forward(self, z):\n        return self.generator(z)\n\n    def compute_gradient_penalty(self, real_samples, fake_samples):\n        \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n        # Random weight term for interpolation between real and fake samples\n        alpha = torch.Tensor(np.random.random((real_samples.size(0), 1, 1, 1))).to(\n            self.device\n        )\n        # Get random interpolation between real and fake samples\n        interpolates = (\n            alpha * real_samples + ((1 - alpha) * fake_samples)\n        ).requires_grad_(True)\n        interpolates = interpolates.to(self.device)\n        d_interpolates = self.discriminator(interpolates)\n        fake = torch.Tensor(real_samples.shape[0], 1).fill_(1.0).to(self.device)\n        # Get gradient w.r.t. interpolates\n        gradients = torch.autograd.grad(\n            outputs=d_interpolates,\n            inputs=interpolates,\n            grad_outputs=fake,\n            create_graph=True,\n            retain_graph=True,\n            only_inputs=True,\n        )[0]\n        gradients = gradients.view(gradients.size(0), -1).to(self.device)\n        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n        return gradient_penalty\n    \n    def adversarial_loss(self, y_hat, y):\n        return nn.BCELoss()(y_hat, y)\n\n    def training_step(self, batch, batch_idx, optimizer_idx):\n        imgs, descriptors = batch\n\n        lambda_gp = 10\n\n        # train generator\n        if optimizer_idx == 0:\n            fake_imgs = self(descriptors)\n            valid = torch.ones(imgs.size(0), 1)\n            valid = valid.type_as(imgs)\n\n            g_loss = -torch.mean(self.discriminator(fake_imgs))\n#             print(self.discriminator(descriptors).shape, valid.shape)\n#             g_loss = self.adversarial_loss(self.discriminator(self(descriptors)), valid)\n            metrics = {\"g_loss\": g_loss}\n            self.log_dict(\n                metrics,\n                on_step=False,\n                on_epoch=True,\n            )\n            return {\"loss\": g_loss}\n\n        # train discriminator\n        elif optimizer_idx == 1:\n            fake_imgs = self(descriptors)\n            real_validity = self.discriminator(imgs)\n            fake_validity = self.discriminator(fake_imgs)\n            \n            valid = torch.ones(imgs.size(0), 1)\n            valid = valid.type_as(imgs)\n            real_loss = self.adversarial_loss(real_validity, valid)\n            \n            fake = torch.zeros(imgs.size(0), 1)\n            fake = fake.type_as(imgs)\n            fake_loss = self.adversarial_loss(fake_validity, fake)\n            \n            d_loss = (real_loss + fake_loss) / 2\n#             gradient_penalty = self.compute_gradient_penalty(imgs.data, fake_imgs.data)\n#             d_loss = (\n#                 -torch.mean(real_validity)\n#                 + torch.mean(fake_validity)\n#                 + lambda_gp * gradient_penalty\n#             )\n            metrics = {\n                \"real_discriminator\": real_loss,\n                \"fake_discriminator\": fake_loss,\n#                 \"penalty_discriminator\": lambda_gp * gradient_penalty,\n                \"d_loss\": d_loss,\n            }\n            self.log_dict(\n                metrics,\n                on_step=False,\n                on_epoch=True,\n            )\n            return {\"loss\": d_loss}\n\n    def configure_optimizers(self):\n        opt_g = torch.optim.Adam(\n            self.generator.parameters(),\n            lr=self.config[\"learning_rate_generator\"],\n            betas=(self.config[\"beta1\"], self.config[\"beta2\"]),\n        )\n        opt_d = torch.optim.Adam(\n            self.discriminator.parameters(),\n            lr=self.config[\"learning_rate_discriminator\"],\n            betas=(self.config[\"beta1\"], self.config[\"beta2\"]),\n        )\n        return (\n            {\"optimizer\": opt_g, \"frequency\": 1},\n            {\"optimizer\": opt_d, \"frequency\": self.config[\"n_critic\"]},\n        )\n\nmodel = WGANGP(config)\nprint(model)\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"[INFO]: {total_params:,} total parameters.\")\nmodel(torch.rand((1, 28))).shape\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T20:24:38.637206Z","iopub.execute_input":"2022-05-05T20:24:38.637697Z","iopub.status.idle":"2022-05-05T20:24:41.144317Z","shell.execute_reply.started":"2022-05-05T20:24:38.637662Z","shell.execute_reply":"2022-05-05T20:24:41.14311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checkpoint\n\nWe had 2 checkpoints to our training:\n\n* one for saving our model every time we have a minimum in the validation loss \n* one for saving the model's and data module script","metadata":{}},{"cell_type":"code","source":"# model_checkpoint = pl.callbacks.model_checkpoint.ModelCheckpoint(\n#     dirpath=run.dir if \"run\" in locals() else \"tmp/\",\n#     filename=\"{epoch}-{val_loss:.3f}\",\n#     monitor=\"val_loss\",\n#     mode=\"min\",\n#     verbose=True,\n#     save_last=True,\n# )\n\nscript_checkpoint = training.ScriptCheckpoint(\n    dirpath=run.dir if \"run\" in locals() else \"tmp/\",\n)\nimages_callback = training.GeneratedImagesCallback(\n    descriptors=first_batch[1].to(device), log_every_n_epochs=10\n)\ncallbacks = [script_checkpoint, images_callback]\nlog = None\nif config[\"job_type\"] == \"train\":\n    callbacks.append(model_checkpoint)\n    print(f\"[INFO]: saving models.\")\nelse:\n    print(f\"[INFO]: not saving models.\")\nif config[\"job_type\"] == \"debug\":\n    log = \"all\"\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T20:24:41.146004Z","iopub.execute_input":"2022-05-05T20:24:41.146482Z","iopub.status.idle":"2022-05-05T20:24:41.163588Z","shell.execute_reply.started":"2022-05-05T20:24:41.146437Z","shell.execute_reply":"2022-05-05T20:24:41.162537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training\n\nWe then train our model.","metadata":{}},{"cell_type":"code","source":"if config[\"log_wandb\"]:\n    wandb_logger = pl.loggers.WandbLogger()\n    wandb_logger.watch(model, log=log, log_graph=True)\nelse:\n    wandb_logger = None\ntrainer = pl.Trainer(\n    max_epochs=4000,\n    callbacks=callbacks,\n    logger=wandb_logger,\n    devices=\"auto\",\n    accelerator=\"auto\",\n    #     limit_train_batches=0.3,\n    #     limit_val_batches=1,\n    #     log_every_n_steps=1,\n)\ntrainer.fit(\n    model,\n    datamodule=dm,\n)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T20:24:41.164677Z","iopub.execute_input":"2022-05-05T20:24:41.165029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs = model(dm.validation_dataset[:60][1].to(device))\nprint(outputs.shape)\ngrid_img = utils.make_grid(outputs, normalize=True, pad_value=1, padding=1)\nplt.figure(figsize=(30, 30))\nplt.imshow(grid_img.permute(1, 2, 0).cpu())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs = torch.permute(model(torch.unsqueeze(dm.validation_dataset[0][1], 0)), (0, 2, 3, 1))\nplt.figure(figsize=(10, 10))\nplt.imshow(np.squeeze(outputs.detach().numpy()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}