{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Simple CNN network\n","\n","The original problem is:\n","\n","    (P) Compute the fabrics from multiple sliced images per rev\n","\n","We are firstly having an intermediate objective:\n","\n","    (P') Compute the fabrics from one single sliced images per rev\n","\n","Of course, the accuracy will be much worse because the fabrics are computed in every direction. However, it is a good try."]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-02-18T18:22:35.71475Z","iopub.status.busy":"2022-02-18T18:22:35.714432Z","iopub.status.idle":"2022-02-18T18:22:35.719176Z","shell.execute_reply":"2022-02-18T18:22:35.718188Z","shell.execute_reply.started":"2022-02-18T18:22:35.714718Z"}},"source":["# Importing the dataframe\n","\n","Firstly, we initialize wandb. It is a tool that allows to store the losses and retrieve the deframe. Otherwise, you can directly access locally the dataframe on your computer."]},{"cell_type":"code","execution_count":46,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-02-22T15:45:37.748960Z","iopub.status.busy":"2022-02-22T15:45:37.748688Z","iopub.status.idle":"2022-02-22T15:45:45.683970Z","shell.execute_reply":"2022-02-22T15:45:45.683134Z","shell.execute_reply.started":"2022-02-22T15:45:37.748930Z"},"trusted":true},"outputs":[],"source":["!pip install wandb --upgrade"]},{"cell_type":"markdown","metadata":{},"source":["We import all the useful packages."]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2022-02-22T15:45:45.687886Z","iopub.status.busy":"2022-02-22T15:45:45.687652Z","iopub.status.idle":"2022-02-22T15:45:45.701964Z","shell.execute_reply":"2022-02-22T15:45:45.698505Z","shell.execute_reply.started":"2022-02-22T15:45:45.687860Z"},"trusted":true},"outputs":[],"source":["import sys\n","from pathlib import Path\n","\n","IS_COLAB = \"google.colab\" in sys.modules\n","IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n","if IS_KAGGLE:\n","    repo_path = Path(\"../input/microstructure-reconstruction\")\n","elif IS_COLAB:\n","    from google.colab import drive\n","\n","    drive.mount(\"/content/gdrive\")\n","    repo_path = Path(\"/content/gdrive/MyDrive/microstructure-reconstruction\")\n","else:\n","    repo_path = Path(\"/home/matias/microstructure-reconstruction\")\n","sys.path.append(str(repo_path))\n","\n","from copy import deepcopy\n","from importlib import reload\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import pytorch_lightning as pl\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchmetrics\n","import torchvision.models as models\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from torch.utils.data import DataLoader\n","from torchvision import transforms, utils\n","from tqdm import tqdm\n","\n","import wandb\n","from custom_datasets import dataset\n","from tools import dataframe_reformat, inspect_code, plotting, training, wandb_api\n","\n","log_wandb = True\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","kwargs = {\"num_workers\": 2, \"pin_memory\": True} if use_cuda else {\"num_workers\": 4}\n","print(f\"[INFO]: Computation device: {device}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["We initialize a wandb run, that will save our metrics"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2022-02-22T15:45:45.703882Z","iopub.status.busy":"2022-02-22T15:45:45.703628Z","iopub.status.idle":"2022-02-22T15:45:52.720143Z","shell.execute_reply":"2022-02-22T15:45:52.719338Z","shell.execute_reply.started":"2022-02-22T15:45:45.703848Z"},"trusted":true},"outputs":[],"source":["if log_wandb:\n","    import wandb\n","\n","    wandb_api.login()\n","    run = wandb.init(\n","        project=\"microstructure-reconstruction\",\n","        group=\"Naive Network\",\n","        job_type=\"train\",\n","    )\n"]},{"cell_type":"markdown","metadata":{},"source":["Parameters of our run:"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2022-02-22T15:45:52.723553Z","iopub.status.busy":"2022-02-22T15:45:52.723077Z","iopub.status.idle":"2022-02-22T15:45:52.743718Z","shell.execute_reply":"2022-02-22T15:45:52.743124Z","shell.execute_reply.started":"2022-02-22T15:45:52.723509Z"},"trusted":true},"outputs":[],"source":["if log_wandb:\n","    config = wandb.config\n","else:\n","    config = {}\n","\n","config[\"job_type\"] = run.job_type\n","config[\"train_val_split\"] = 0.7\n","config[\"seed\"] = 42\n","config[\"batch_size\"] = 64\n","config[\"learning_rate\"] = 0.0001\n","config[\"device\"] = device\n","config[\"momentum\"] = 0.9\n","config[\"architecture\"] = \"VGG\"\n","config[\"input_width\"] = 64\n","config[\"weight_decay\"] = 0.0000\n","config[\"epochs\"] = 0\n","config[\"frac_sample\"] = 1\n","config[\"log_wandb\"] = log_wandb\n","torch.manual_seed(config[\"seed\"])\n","pl.seed_everything(config[\"seed\"])\n"]},{"cell_type":"markdown","metadata":{},"source":["# Datasets\n","\n","We create the training and validation dataset from our dataframe of descriptors. "]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2022-02-22T15:45:52.750359Z","iopub.status.busy":"2022-02-22T15:45:52.749699Z","iopub.status.idle":"2022-02-22T15:45:53.124101Z","shell.execute_reply":"2022-02-22T15:45:53.123328Z","shell.execute_reply.started":"2022-02-22T15:45:52.750293Z"},"trusted":true},"outputs":[],"source":["class DataModule(pl.LightningDataModule):\n","    def __init__(\n","        self,\n","        config,\n","        repo_path,\n","    ):\n","        super().__init__()\n","        self.config = config\n","\n","        if self.config[\"log_wandb\"]:\n","            self.data_at = wandb.Api().artifact(\n","                \"matiasetcheverry/microstructure-reconstruction/\"\n","                + \"raw_fabrics\"\n","                + \":3_images\"\n","            )\n","\n","        self.transform = transforms.Compose(\n","            [\n","                transforms.CenterCrop(207),\n","                transforms.Resize(\n","                    (self.config[\"input_width\"], self.config[\"input_width\"])\n","                ),\n","                transforms.ToTensor(),\n","                transforms.GaussianBlur(kernel_size=3, sigma=0.5),\n","            ]\n","        )\n","\n","    def prepare_data(self):\n","        if self.config[\"log_wandb\"]:\n","            self.data_at.download()\n","\n","    def init_fabrics_df(self):\n","        if self.config[\"log_wandb\"]:\n","            self.fabrics_df = wandb_api.convert_table_to_dataframe(\n","                self.data_at.get(\"fabrics\")\n","            )\n","        else:\n","            self.fabrics_df = pd.read_csv(repo_path / \"REV1_600/fabrics.txt\")\n","            path_to_slices = repo_path / \"REV1_600/REV1_600Slices\"\n","            self.fabrics_df[\"photos\"] = self.fabrics_df[\"id\"].apply(\n","                func=dataframe_reformat.associate_rev_id_to_its_images,\n","                args=(path_to_slices, 3, repo_path),\n","            )\n","        self.fabrics_df[\"photos\"] = self.fabrics_df[\"photos\"].apply(\n","            func=lambda photo_paths: [str(repo_path / Path(x)) for x in photo_paths]\n","        )\n","        self.fabrics_df = self.fabrics_df[self.fabrics_df.photos.str.len().gt(0)]\n","        self.fabrics_df = self.fabrics_df.sample(\n","            frac=self.config[\"frac_sample\"], random_state=self.config[\"seed\"]\n","        )\n","        self.single_fabrics_df = dataframe_reformat.convert_into_single_entry_df(\n","            self.fabrics_df, \"photos\"\n","        )\n","\n","    def setup(self, stage):\n","        self.init_fabrics_df()\n","        self.scaler = MinMaxScaler(feature_range=(0, 1))\n","        normalized_fabrics = deepcopy(self.single_fabrics_df)\n","        normalized_fabrics.iloc[:, 1:-1] = self.scaler.fit_transform(\n","            self.single_fabrics_df.iloc[:, 1:-1]\n","        )\n","\n","        train_df, test_df = train_test_split(\n","            normalized_fabrics,\n","            train_size=self.config[\"train_val_split\"],\n","            random_state=self.config[\"seed\"],\n","            shuffle=True,\n","        )\n","\n","        self.train_dataset = dataset.SinglePhotoDataset(\n","            train_df, transform=self.transform\n","        )\n","        self.validation_dataset = dataset.SinglePhotoDataset(\n","            test_df,\n","            transform=self.transform,\n","        )\n","        self.targets = test_df.iloc[:, 1:-1].to_numpy()\n","\n","    def train_dataloader(self):\n","        return DataLoader(\n","            self.train_dataset,\n","            batch_size=self.config[\"batch_size\"],\n","            shuffle=True,\n","            **kwargs,\n","        )\n","\n","    def val_dataloader(self):\n","        return DataLoader(\n","            self.validation_dataset,\n","            batch_size=self.config[\"batch_size\"],\n","            shuffle=False,\n","            **kwargs,\n","        )\n","\n","    def test_dataloader(self):\n","        return self.val_dataloader()\n","\n","    def predict_dataloader(self):\n","        return DataLoader(\n","            [image for image, _ in self.validation_dataset],\n","            batch_size=self.config[\"batch_size\"],\n","            shuffle=False,\n","            **kwargs,\n","        )\n","\n","\n","dm = DataModule(config, repo_path)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Model\n","\n","We then create our model, with a forward method."]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2022-02-22T15:45:53.127026Z","iopub.status.busy":"2022-02-22T15:45:53.126454Z","iopub.status.idle":"2022-02-22T15:45:53.221207Z","shell.execute_reply":"2022-02-22T15:45:53.220519Z","shell.execute_reply.started":"2022-02-22T15:45:53.126984Z"},"trusted":true},"outputs":[],"source":["class VGG11(pl.LightningModule):\n","    def __init__(self, config, scaler=None):\n","        super().__init__()\n","\n","        self.config = config\n","        self.config[\"model_type\"] = type(self)\n","        self.scaler = scaler\n","\n","        self.configure_model()\n","        self.configure_criterion()\n","        self.configure_metrics()\n","\n","    def configure_model(self):\n","        # convolutional layers\n","        self.conv_layers = nn.Sequential(\n","            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","        )\n","        input_fc = int((self.config[\"input_width\"] / (2 ** 5)) ** 2 * 512)\n","        # fully connected linear layers\n","        self.linear_layers = nn.Sequential(\n","            nn.Linear(in_features=input_fc, out_features=512),\n","            nn.ReLU(),\n","            nn.Dropout2d(0.5),\n","            nn.Linear(in_features=512, out_features=512),\n","            nn.ReLU(),\n","            nn.Dropout2d(0.5),\n","            nn.Linear(in_features=512, out_features=23),\n","        )\n","\n","    def forward(self, x):\n","        x = self.conv_layers(x)\n","        # flatten to prepare for the fully connected layers\n","        x = x.view(x.size(0), -1)\n","        x = self.linear_layers(x)\n","        return x\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y = batch\n","        y_hat = self(x)\n","        loss = self.criterion(y_hat, y)\n","        self.log(\n","            \"train_loss\",\n","            loss,\n","            on_step=False,\n","            on_epoch=True,\n","        )\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y = batch\n","        y_hat = self(x)\n","        metrics = {name: metric(y, y_hat) for name, metric in self.metrics.items()}\n","        self.log_dict(metrics, on_step=False, on_epoch=True)\n","        return metrics\n","\n","    def predict_step(self, batch, batch_idx: int, dataloader_idx: int = None):\n","        return self(batch)\n","\n","    def training_epoch_end(self, outputs):\n","        self.config[\"epochs\"] += 1\n","\n","    def configure_criterion(self):\n","        self.criterion = nn.L1Loss()\n","        self.config[\"loss_type\"] = type(self.criterion)\n","\n","    def configure_metrics(self):\n","        self.metrics = {\n","            \"val_loss\": self.criterion,\n","            \"mae\": torchmetrics.MeanAbsoluteError().to(self.config[\"device\"]),\n","            \"mape\": torchmetrics.MeanAbsolutePercentageError().to(\n","                self.config[\"device\"]\n","            ),\n","            \"smape\": torchmetrics.SymmetricMeanAbsolutePercentageError().to(\n","                self.config[\"device\"]\n","            ),\n","            \"r2_score\": torchmetrics.R2Score(num_outputs=23).to(self.config[\"device\"]),\n","            \"cosine_similarity\": torchmetrics.CosineSimilarity(reduction=\"mean\").to(\n","                self.config[\"device\"]\n","            ),\n","        }\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(\n","            self.parameters(),\n","            lr=self.config[\"learning_rate\"],\n","            weight_decay=self.config[\"weight_decay\"],\n","        )\n","        self.config[\"optimizer_type\"] = type(optimizer)\n","        return optimizer\n","\n","\n","model = VGG11(config)\n","total_params = sum(p.numel() for p in model.parameters())\n","print(f\"[INFO]: {total_params:,} total parameters.\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Checkpoint\n","\n","We had 2 checkpoints to our training:\n","\n","* one for saving our model every time we have a minimum in the validation loss \n","* one for saving the model's and data module script"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2022-02-22T15:45:53.222715Z","iopub.status.busy":"2022-02-22T15:45:53.222473Z","iopub.status.idle":"2022-02-22T15:45:53.234463Z","shell.execute_reply":"2022-02-22T15:45:53.233672Z","shell.execute_reply.started":"2022-02-22T15:45:53.222682Z"},"trusted":true},"outputs":[],"source":["model_checkpoint = pl.callbacks.model_checkpoint.ModelCheckpoint(\n","    dirpath=run.dir,\n","    filename=\"{epoch}-{val_loss:.3f}\",\n","    monitor=\"val_loss\",\n","    mode=\"min\",\n","    verbose=True,\n","    save_last=True,\n",")\n","\n","script_checkpoint = training.ScriptCheckpoint(\n","    dirpath=run.dir,\n",")\n","\n","callbacks = [script_checkpoint]\n","log = None\n","if run.job_type == \"train\":\n","    callbacks.append(model_checkpoint)\n","    print(f\"[INFO]: saving models.\")\n","if run.job_type == \"debug\":\n","    log = \"all\"\n"]},{"cell_type":"markdown","metadata":{},"source":["# Training\n","\n","We then train our model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-22T15:45:53.236095Z","iopub.status.busy":"2022-02-22T15:45:53.235453Z"},"trusted":true},"outputs":[],"source":["if config[\"log_wandb\"]:\n","    wandb_logger = pl.loggers.WandbLogger()\n","    wandb_logger.watch(model, log=log, log_graph=True)\n","else:\n","    wandb_logger = None\n","trainer = pl.Trainer(\n","    max_epochs=150,\n","    callbacks=callbacks,\n","    logger=wandb_logger,\n","    devices=\"auto\",\n","    accelerator=\"auto\",\n","    #     limit_train_batches=0.1,\n","    #     limit_val_batches=0.1,\n","    #     log_every_n_steps=1\n",")\n","trainer.fit(\n","    model,\n","    datamodule=dm,\n",")\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.8"}},"nbformat":4,"nbformat_minor":4}
